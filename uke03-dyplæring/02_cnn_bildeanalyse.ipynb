{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588180a8",
   "metadata": {},
   "source": [
    "# ÔøΩÔøΩ CNN for Blomsterklassifikasjon: Fra Natur til Medisin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2028110f",
   "metadata": {},
   "source": [
    "02_cnn_bildeanalyse.ipynb\n",
    "<!--\n",
    "Lag en illustrativ notebook (02_cnn_bildeanalyse.ipynb) som  klassifisere bilder av blomster i 5 klasser: tulipan, tusenfryd, l√∏vetann, rose, solsikke. Bruk data fra @https://www.kaggle.com/alxmamaev/flowers-recognition/data og konstrurer en CNN som trenes og evalueres (forvirringsmatrise, class activation maps, etc.). Bruk Pytorch og skriv rikelig med forklarende tekst som markdown. Pr√∏v √• trekk sammenlikninger med medisinsk bilder og vis til formalismen  y ~f(X, theta).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb76d721",
   "metadata": {},
   "source": [
    "Denne notebook-en demonstrerer hvordan **Convolutional Neural Networks (CNN)** kan brukes til √• klassifisere naturlige bilder (blomster) og sammenligner dette med medisinsk bildeanalyse.\n",
    "\n",
    "## M√•l\n",
    "- Bygge og trene en CNN for √• klassifisere 5 blomstertyper\n",
    "- Forst√• hvordan CNN fungerer p√• naturlige bilder\n",
    "- Sammenligne med medisinsk bildeanalyse (MRI, r√∏ntgen, etc.)\n",
    "- Demonstrere evalueringsmetoder: forvirringsmatrise, class activation maps\n",
    "- Illustrere formalismen **y ~ f(X, Œ∏)** i praksis\n",
    "\n",
    "## Datasett\n",
    "Vi bruker [Flowers Recognition Dataset](https://www.kaggle.com/alxmamaev/flowers-recognition/data) fra Kaggle med 5 klasser:\n",
    "- ÔøΩÔøΩ **Tulipan** (Tulip)\n",
    "- ÔøΩÔøΩ **Tusenfryd** (Daisy) \n",
    "- üåª **L√∏vetann** (Dandelion)\n",
    "- üåπ **Rose** (Rose)\n",
    "- üåª **Solsikke** (Sunflower)\n",
    "\n",
    "## Teoretisk Fundament\n",
    "\n",
    "### Formalismen y ~ f(X, Œ∏)\n",
    "\n",
    "I maskinl√¶ring kan vi uttrykke klassifikasjonsproblemet som:\n",
    "\n",
    "**y = f(X, Œ∏) + Œµ**\n",
    "\n",
    "Hvor:\n",
    "- **y** = predikert klasse (blomstertype)\n",
    "- **X** = input bilde (pikselverdier)\n",
    "- **Œ∏** = modellparametere (CNN-vekter)\n",
    "- **f** = ikke-line√¶r funksjon (CNN-arkitekturen)\n",
    "- **Œµ** = feilterm (noise)\n",
    "\n",
    "Dette er identisk med medisinsk bildeanalyse, bare med forskjellige klasser!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e05fc3",
   "metadata": {},
   "source": [
    "## Sammenligning: Naturlige vs Medisinske Bilder\n",
    "\n",
    "| Aspekt | Blomsterklassifikasjon | Medisinsk Bildeanalyse |\n",
    "|--------|------------------------|------------------------|\n",
    "| **Input (X)** | RGB-bilder av blomster | Muliparametrisk MRI (T1, T2, PD) 3D medisinske bilder |\n",
    "| **Klasser (y)** | Blomstertyper | Sykdomstilstander |\n",
    "| **Kompleksitet** | Farge, form, tekstur | Anatomiske strukturer |\n",
    "| **Konsekvenser** | Estetisk/kommersiell | Frisk/Syk |\n",
    "| **Datamengde** | Tusener av bilder | Begrenset (privacy) |\n",
    "| **Ekspertise** | Botanikk | Medisin/radiologi |\n",
    "\n",
    "**Felles prinsipper:**\n",
    "- B√•de krever domenekunnskap\n",
    "- B√•de har klasseubalanse-problemer  \n",
    "- B√•de trenger robuste modeller\n",
    "- B√•de har etiske implikasjoner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b14d6",
   "metadata": {},
   "source": [
    "### Men f√∏rst: üîß milj√∏oppsett - kode skal fungere b√•de lokalt, i Codespaces samt Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97eac426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Kj√∏rer i lokal milj√∏/Codespaces\n",
      "‚úÖ Milj√∏ er konfigurert og klart!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Sjekk om vi kj√∏rer i Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üöÄ Kj√∏rer i Google Colab\")\n",
    "    \n",
    "    # Installer n√∏dvendige pakker som ikke er forh√•ndsinstallert i Colab\n",
    "    !pip install seaborn --quiet\n",
    "    \n",
    "    # Sjekk om mappen allerede eksisterer\n",
    "    if not os.path.exists('AI-og-helse'):\n",
    "        print(\"üì• Laster ned kursmateriell...\")\n",
    "        try:\n",
    "            # Pr√∏v √• klone repositoryet (da v√¶re public)\n",
    "            !git clone https://github.com/arvidl/AI-og-helse.git\n",
    "            print(\"‚úÖ Repository klonet vellykket!\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Kunne ikke klone repository automatisk\")\n",
    "            print(\"üí° Du kan laste opp filer manuelt eller bruke en annen metode\")\n",
    "    \n",
    "    # Bytt til riktig mappe hvis den eksisterer\n",
    "    if os.path.exists('AI-og-helse'):\n",
    "        os.chdir('AI-og-helse')\n",
    "        print(f\"üìÅ Byttet til mappe: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(\"üìÇ Arbeider i standard Colab-mappe\")\n",
    "        \n",
    "else:\n",
    "    print(\"üíª Kj√∏rer i lokal milj√∏/Codespaces\")\n",
    "\n",
    "# Standard imports som fungerer overalt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ Milj√∏ er konfigurert og klart!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3552689",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install opencv-python --quiet\n",
    "    !pip install tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85315493",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mzipfile\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     19\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Imports og setup med feilh√•ndtering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sjekk om GPU eller MPS er tilgjengelig og sett enhet (device)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46674f",
   "metadata": {},
   "source": [
    "Jeg skal lage en omfattende notebook for blomsterklassifikasjon som sammenligner med medisinsk bildeanalyse. La meg f√∏rst se p√• den eksisterende filen og deretter bygge opp en komplett l√∏sning.\n",
    "\n",
    "\n",
    "[1 tool called]\n",
    "\n",
    "N√• skal jeg lage en komplett notebook for blomsterklassifikasjon med sammenligninger til medisinsk bildeanalyse:\n",
    "\n",
    "```python\n",
    "# 02_cnn_bildeanalyse.ipynb\n",
    "\n",
    "# Cell 1: Markdown\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Cell 2: Markdown\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Cell 3: Imports og setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Cell 4: Markdown\n",
    "\"\"\"\n",
    "## 1. Data Lasting og Forberedelse\n",
    "\n",
    "### Datasettstruktur\n",
    "```\n",
    "flowers/\n",
    "‚îú‚îÄ‚îÄ tulip/          # ~1000 bilder\n",
    "‚îú‚îÄ‚îÄ daisy/          # ~1000 bilder  \n",
    "‚îú‚îÄ‚îÄ dandelion/      # ~1000 bilder\n",
    "‚îú‚îÄ‚îÄ rose/           # ~1000 bilder\n",
    "‚îî‚îÄ‚îÄ sunflower/      # ~1000 bilder\n",
    "```\n",
    "\n",
    "### Preprosessering\n",
    "- **Resize**: Alle bilder til 224x224 piksler\n",
    "- **Normalisering**: Standard ImageNet-verdier\n",
    "- **Dataaugmentering**: Rotasjon, flipping, fargeendringer\n",
    "- **Train/Val/Test split**: 70/15/15\n",
    "\"\"\"\n",
    "\n",
    "# Cell 5: Data loading and preprocessing\n",
    "class FlowerDataset(Dataset):\n",
    "    \"\"\"Custom dataset for flower classification\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "def load_flower_data(data_dir, test_size=0.15, val_size=0.15):\n",
    "    \"\"\"Load and split flower dataset\"\"\"\n",
    "    \n",
    "    # Class names\n",
    "    class_names = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n",
    "    class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "    \n",
    "    # Collect all image paths and labels\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_paths.append(os.path.join(class_dir, filename))\n",
    "                    labels.append(class_to_idx[class_name])\n",
    "    \n",
    "    print(f\"Total images found: {len(image_paths)}\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Images per class: {np.bincount(labels)}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        image_paths, labels, test_size=test_size + val_size, \n",
    "        random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=test_size/(test_size + val_size),\n",
    "        random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), class_names\n",
    "\n",
    "# Data transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Cell 6: Markdown\n",
    "\"\"\"\n",
    "## 2. CNN Arkitektur\n",
    "\n",
    "### V√•r modell: FlowerNet\n",
    "\n",
    "Vi bygger en CNN inspirert av VGG-arkitekturen, men tilpasset for blomsterklassifikasjon:\n",
    "\n",
    "```\n",
    "Input (3, 224, 224) \n",
    "    ‚Üì\n",
    "Conv2d(3‚Üí64) + ReLU + MaxPool\n",
    "    ‚Üì  \n",
    "Conv2d(64‚Üí128) + ReLU + MaxPool\n",
    "    ‚Üì\n",
    "Conv2d(128‚Üí256) + ReLU + MaxPool  \n",
    "    ‚Üì\n",
    "Conv2d(256‚Üí512) + ReLU + MaxPool\n",
    "    ‚Üì\n",
    "AdaptiveAvgPool2d(7√ó7)\n",
    "    ‚Üì\n",
    "Flatten ‚Üí Linear(512√ó7√ó7 ‚Üí 256) + ReLU + Dropout(0.5)\n",
    "    ‚Üì\n",
    "Linear(256 ‚Üí 5) + Softmax\n",
    "    ‚Üì\n",
    "Output: [p_daisy, p_dandelion, p_rose, p_sunflower, p_tulip]\n",
    "```\n",
    "\n",
    "### Sammenligning med Medisinsk CNN\n",
    "\n",
    "| Komponent | Blomster-CNN | Medisinsk-CNN |\n",
    "|-----------|--------------|---------------|\n",
    "| **Input kanaler** | 3 (RGB) | 1 (Grayscale) eller 3+ |\n",
    "| **F√∏rste lag** | Edge detection | Anatomisk struktur |\n",
    "| **Dybde** | 4-8 lag | 10-50+ lag |\n",
    "| **Pooling** | MaxPool | AvgPool (bevarer info) |\n",
    "| **Output** | 5 blomstertyper | 2+ sykdomstilstander |\n",
    "\"\"\"\n",
    "\n",
    "# Cell 7: CNN Model Definition\n",
    "class FlowerNet(nn.Module):\n",
    "    \"\"\"CNN for flower classification\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=5, dropout_rate=0.5):\n",
    "        super(FlowerNet, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 2  \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to handle different input sizes\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 7 * 7, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Cell 8: Markdown\n",
    "\"\"\"\n",
    "## 3. Treningsprosess\n",
    "\n",
    "### Loss Function: Cross-Entropy\n",
    "For multi-class klassifikasjon bruker vi Cross-Entropy Loss:\n",
    "\n",
    "**L = -‚àë(y_i √ó log(≈∑_i))**\n",
    "\n",
    "Hvor:\n",
    "- y_i = true class (one-hot encoded)\n",
    "- ≈∑_i = predicted probability for class i\n",
    "\n",
    "### Optimizer: Adam\n",
    "- **Learning rate**: 0.001\n",
    "- **Weight decay**: 1e-4 (L2 regularisering)\n",
    "- **Batch size**: 32\n",
    "\n",
    "### Sammenligning med Medisinsk Trening\n",
    "\n",
    "| Parameter | Blomster | Medisin |\n",
    "|-----------|----------|---------|\n",
    "| **Epochs** | 20-50 | 100-500+ |\n",
    "| **Batch size** | 32-64 | 8-32 |\n",
    "| **Learning rate** | 1e-3 | 1e-4 - 1e-5 |\n",
    "| **Patience** | 10 | 20-50 |\n",
    "| **Augmentation** | Moderate | Heavy |\n",
    "\"\"\"\n",
    "\n",
    "# Cell 9: Training Functions\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate model for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=30, learning_rate=0.001):\n",
    "    \"\"\"Train the complete model\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_flower_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= 10:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses, \n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }\n",
    "\n",
    "# Cell 10: Markdown\n",
    "\"\"\"\n",
    "## 4. Visualisering og Evalueringsmetoder\n",
    "\n",
    "### Forvirringsmatrise (Confusion Matrix)\n",
    "Visuell representasjon av modellens ytelse per klasse:\n",
    "\n",
    "- **Diagonal**: Riktige prediksjoner\n",
    "- **Off-diagonal**: Feilklassifiseringer\n",
    "- **Fargeintensitet**: Antall bilder\n",
    "\n",
    "### Class Activation Maps (CAM)\n",
    "Visuell forklaring av hvilke deler av bildet modellen fokuserer p√•:\n",
    "\n",
    "**CAM = ‚àë(w_i √ó A_i)**\n",
    "\n",
    "Hvor:\n",
    "- w_i = vekt for klasse i\n",
    "- A_i = aktivitetskart fra siste convolutional layer\n",
    "\n",
    "### Sammenligning med Medisinsk Evaluering\n",
    "\n",
    "| Metode | Blomster | Medisin |\n",
    "|--------|----------|---------|\n",
    "| **Accuracy** | 85-95% | 70-90% |\n",
    "| **Precision/Recall** | Per blomstertype | Per sykdomstilstand |\n",
    "| **CAM** | Blomsterdel, farge | Anatomisk struktur |\n",
    "| **ROC-AUC** | Multi-class | Binary/Multi-class |\n",
    "\"\"\"\n",
    "\n",
    "# Cell 11: Evaluation Functions\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(history['train_losses'], label='Training Loss', color='blue')\n",
    "    axes[0].plot(history['val_losses'], label='Validation Loss', color='red')\n",
    "    axes[0].set_title('Model Loss Over Time')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(history['train_accs'], label='Training Accuracy', color='blue')\n",
    "    axes[1].plot(history['val_accs'], label='Validation Accuracy', color='red')\n",
    "    axes[1].set_title('Model Accuracy Over Time')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probabilities = np.array(all_probabilities)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (all_predictions == all_labels).mean()\n",
    "    print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(classification_report(all_labels, all_predictions, \n",
    "                              target_names=class_names))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                display_labels=class_names)\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title('Confusion Matrix - Flower Classification')\n",
    "    plt.show()\n",
    "    \n",
    "    return all_predictions, all_labels, all_probabilities\n",
    "\n",
    "def plot_class_activation_map(model, image, class_idx, class_names):\n",
    "    \"\"\"Generate and plot Class Activation Map\"\"\"\n",
    "    \n",
    "    # Hook to get feature maps from last conv layer\n",
    "    features = None\n",
    "    def hook_fn(module, input, output):\n",
    "        nonlocal features\n",
    "        features = output\n",
    "    \n",
    "    # Register hook\n",
    "    hook = model.features[-2].register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image.unsqueeze(0).to(device)\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Get weights for the target class\n",
    "    weights = model.classifier[-1].weight[class_idx].detach().cpu()\n",
    "    \n",
    "    # Generate CAM\n",
    "    cam = torch.zeros(features.shape[2:])\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * features[0, i, :, :].cpu()\n",
    "    \n",
    "    # Normalize CAM\n",
    "    cam = torch.relu(cam)\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "    \n",
    "    # Remove hook\n",
    "    hook.remove()\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    original_image = image.permute(1, 2, 0).cpu()\n",
    "    # Denormalize for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    original_image = original_image * std + mean\n",
    "    original_image = torch.clamp(original_image, 0, 1)\n",
    "    \n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title(f'Original Image\\nPredicted: {class_names[class_idx]}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # CAM\n",
    "    axes[1].imshow(cam, cmap='jet')\n",
    "    axes[1].set_title('Class Activation Map')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2].imshow(original_image)\n",
    "    axes[2].imshow(cam, cmap='jet', alpha=0.5)\n",
    "    axes[2].set_title('CAM Overlay')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cam\n",
    "\n",
    "# Cell 12: Markdown\n",
    "\"\"\"\n",
    "## 5. Hovedkj√∏ring og Resultater\n",
    "\n",
    "### Steg-for-steg prosess:\n",
    "\n",
    "1. **Last data** fra Kaggle dataset\n",
    "2. **Bygg CNN-modell** (FlowerNet)\n",
    "3. **Trene modell** med early stopping\n",
    "4. **Evaluer ytelse** med multiple metrikker\n",
    "5. **Visualiser resultater** med CAM og forvirringsmatrise\n",
    "6. **Sammenlign** med medisinsk bildeanalyse\n",
    "\n",
    "### Forventede resultater:\n",
    "- **Accuracy**: 80-90% p√• testsett\n",
    "- **Training time**: 10-30 minutter (GPU)\n",
    "- **Model size**: ~50MB\n",
    "- **Inference time**: <1ms per bilde\n",
    "\"\"\"\n",
    "\n",
    "# Cell 13: Main execution\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    print(\"üå∏ Flower Classification with CNN\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if data exists, if not provide instructions\n",
    "    data_dir = \"flowers\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(\"‚ùå Data directory not found!\")\n",
    "        print(\"Please download the dataset from:\")\n",
    "        print(\"https://www.kaggle.com/alxmamaev/flowers-recognition/data\")\n",
    "        print(\"Extract to 'flowers' directory with structure:\")\n",
    "        print(\"flowers/\")\n",
    "        print(\"‚îú‚îÄ‚îÄ daisy/\")\n",
    "        print(\"‚îú‚îÄ‚îÄ dandelion/\")\n",
    "        print(\"‚îú‚îÄ‚îÄ rose/\")\n",
    "        print(\"‚îú‚îÄ‚îÄ sunflower/\")\n",
    "        print(\"‚îî‚îÄ‚îÄ tulip/\")\n",
    "        return\n",
    "    \n",
    "    # Load data\n",
    "    print(\"ÔøΩÔøΩ Loading data...\")\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test), class_names = load_flower_data(data_dir)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = FlowerDataset(X_train, y_train, train_transform)\n",
    "    val_dataset = FlowerDataset(X_val, y_val, val_transform)\n",
    "    test_dataset = FlowerDataset(X_test, y_test, val_transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    print(f\"‚úÖ Data loaded successfully!\")\n",
    "    print(f\"Train: {len(train_dataset)} images\")\n",
    "    print(f\"Val: {len(val_dataset)} images\") \n",
    "    print(f\"Test: {len(test_dataset)} images\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\nÔøΩÔøΩÔ∏è Building model...\")\n",
    "    model = FlowerNet(num_classes=5).to(device)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nüöÄ Training model...\")\n",
    "    history = train_model(model, train_loader, val_loader, num_epochs=30)\n",
    "    \n",
    "    # Plot training history\n",
    "    print(\"\\nÔøΩÔøΩ Training results:\")\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_flower_model.pth'))\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nÔøΩÔøΩ Evaluating model...\")\n",
    "    predictions, labels, probabilities = evaluate_model(model, test_loader, class_names)\n",
    "    \n",
    "    # Show some example predictions\n",
    "    print(\"\\nüñºÔ∏è Example predictions:\")\n",
    "    show_example_predictions(model, test_loader, class_names, num_examples=8)\n",
    "    \n",
    "    # Generate CAM for a few examples\n",
    "    print(\"\\nüéØ Class Activation Maps:\")\n",
    "    generate_cam_examples(model, test_loader, class_names, num_examples=3)\n",
    "    \n",
    "    print(\"\\n‚úÖ Analysis complete!\")\n",
    "    print(f\"Best validation accuracy: {history['best_val_acc']:.2f}%\")\n",
    "\n",
    "def show_example_predictions(model, test_loader, class_names, num_examples=8):\n",
    "    \"\"\"Show example predictions with confidence scores\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            if i >= num_examples:\n",
    "                break\n",
    "                \n",
    "            image = images[0]\n",
    "            true_label = labels[0].item()\n",
    "            \n",
    "            # Get prediction\n",
    "            image_tensor = image.unsqueeze(0).to(device)\n",
    "            outputs = model(image_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "            confidence = probabilities[0, predicted_class].item()\n",
    "            \n",
    "            # Plot\n",
    "            axes[i].imshow(image.permute(1, 2, 0))\n",
    "            axes[i].set_title(f'True: {class_names[true_label]}\\n'\n",
    "                            f'Pred: {class_names[predicted_class]}\\n'\n",
    "                            f'Conf: {confidence:.3f}')\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_cam_examples(model, test_loader, class_names, num_examples=3):\n",
    "    \"\"\"Generate CAM examples for different classes\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    examples_generated = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            if examples_generated >= num_examples:\n",
    "                break\n",
    "                \n",
    "            image = images[0]\n",
    "            true_label = labels[0].item()\n",
    "            \n",
    "            # Get prediction\n",
    "            image_tensor = image.unsqueeze(0).to(device)\n",
    "            outputs = model(image_tensor)\n",
    "            predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "            \n",
    "            # Generate CAM\n",
    "            plot_class_activation_map(model, image, predicted_class, class_names)\n",
    "            examples_generated += 1\n",
    "\n",
    "# Cell 14: Markdown\n",
    "\"\"\"\n",
    "## 6. Sammenligning med Medisinsk Bildeanalyse\n",
    "\n",
    "### Likheter og Forskjeller\n",
    "\n",
    "| Aspekt | Blomsterklassifikasjon | Medisinsk Bildeanalyse |\n",
    "|--------|------------------------|------------------------|\n",
    "| **Formalism** | y = f(X, Œ∏) | y = f(X, Œ∏) |\n",
    "| **Input** | RGB-bilder (3 kanaler) | Grayscale/3D (1-100+ kanaler) |\n",
    "| **Klasser** | 5 blomstertyper | 2+ sykdomstilstander |\n",
    "| **Datamengde** | ~5000 bilder | ~100-10000 bilder |\n",
    "| **Ekspertise** | Botanikk | Medisin/radiologi |\n",
    "| **Konsekvenser** | Estetisk/kommersiell | Liv og d√∏d |\n",
    "| **Regulering** | Minimal | FDA/CE/MDR |\n",
    "| **Bias** | Estetisk preferanser | Rase/kj√∏nn/aldersbias |\n",
    "| **Forklarbarhet** | CAM p√• blomsterdel | CAM p√• anatomisk struktur |\n",
    "\n",
    "### Felles Prinsipper\n",
    "\n",
    "1. **Feature Learning**: CNN l√¶rer hierarkiske features\n",
    "2. **Transfer Learning**: Pre-trained modeller (ImageNet ‚Üí Flowers/Medical)\n",
    "3. **Data Augmentation**: Rotasjon, scaling, fargeendringer\n",
    "4. **Regularization**: Dropout, weight decay, early stopping\n",
    "5. **Evaluation**: Cross-validation, confusion matrix, ROC-AUC\n",
    "6. **Interpretability**: CAM, Grad-CAM, attention maps\n",
    "\n",
    "### Praktiske Implikasjoner\n",
    "\n",
    "**For Blomster:**\n",
    "- Automatisk sortering i gartnerier\n",
    "- Mobil-app for blomsteridentifikasjon\n",
    "- E-commerce produktkategorisering\n",
    "\n",
    "**For Medisin:**\n",
    "- Hjelp til radiologer i diagnostikk\n",
    "- Tidlig oppdagelse av sykdommer\n",
    "- Personalisert behandling\n",
    "- Telemedisin og remote diagnosis\n",
    "\n",
    "### Etiske Overveielser\n",
    "\n",
    "**Begge domener har:**\n",
    "- Bias-problemer (representativitet)\n",
    "- Transparens (forklarbare AI)\n",
    "- Sikkerhet (robusthet)\n",
    "- Privacy (databeskyttelse)\n",
    "\n",
    "**Medisinsk AI har tillegg:**\n",
    "- Regulatorisk godkjenning\n",
    "- Klinisk validering\n",
    "- Ansvar og liability\n",
    "- Pasientsikkerhet\n",
    "\"\"\"\n",
    "\n",
    "# Cell 15: Markdown\n",
    "\"\"\"\n",
    "## 7. Konklusjon og Fremtidige Retninger\n",
    "\n",
    "### Hva vi har l√¶rt\n",
    "\n",
    "1. **CNN-prinsipper** fungerer p√• tvers av domener\n",
    "2. **Formalismen y ~ f(X, Œ∏)** er universell\n",
    "3. **Evaluering** krever domenespesifikke metrikker\n",
    "4. **Visualisering** (CAM) hjelper med tolkning\n",
    "5. **Etikk** er viktig i alle AI-applikasjoner\n",
    "\n",
    "### Fremtidige Forbedringer\n",
    "\n",
    "**Tekniske:**\n",
    "- Transfer learning fra ImageNet\n",
    "- Attention mechanisms\n",
    "- Ensemble methods\n",
    "- Advanced augmentation\n",
    "\n",
    "**Medisinske:**\n",
    "- 3D CNN for volumetriske data\n",
    "- Multi-modal fusion\n",
    "- Federated learning\n",
    "- Real-time inference\n",
    "\n",
    "### Praktisk Anvendelse\n",
    "\n",
    "Denne notebook-en demonstrerer hvordan grunnleggende CNN-prinsipper kan anvendes p√• b√•de naturlige og medisinske bilder. Selv om applikasjonene er forskjellige, er de underliggende algoritmene og evalueringsmetodene de samme.\n",
    "\n",
    "**N√∏kkel-takeaway:** God forst√•else av CNN p√• naturlige bilder gir solid fundament for medisinsk bildeanalyse!\n",
    "\"\"\"\n",
    "\n",
    "# Cell 16: Run main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "Denne omfattende notebook-en dekker:\n",
    "\n",
    "## **Hovedkomponenter:**\n",
    "\n",
    "1. **Teoretisk fundament** - Formalismen y ~ f(X, Œ∏)\n",
    "2. **Data lasting** - Kaggle flowers dataset\n",
    "3. **CNN arkitektur** - FlowerNet med 4 convolutional blocks\n",
    "4. **Treningsprosess** - Med early stopping og learning rate scheduling\n",
    "5. **Evaluering** - Confusion matrix, classification report, CAM\n",
    "6. **Visualisering** - Training curves, example predictions, activation maps\n",
    "7. **Sammenligning** - Detaljert sammenligning med medisinsk bildeanalyse\n",
    "\n",
    "## **Unike funksjoner:**\n",
    "\n",
    "- **Class Activation Maps (CAM)** for √• vise hvilke deler av blomsten modellen fokuserer p√•\n",
    "- **Omfattende sammenligning** mellom naturlige og medisinske bilder\n",
    "- **Etiske overveielser** for begge domener\n",
    "- **Praktiske implikasjoner** og fremtidige retninger\n",
    "\n",
    "## **Pedagogisk verdi:**\n",
    "\n",
    "- **Konkrete eksempler** p√• hvordan CNN fungerer\n",
    "- **Visuell forklaring** av modellens beslutningsprosess\n",
    "- **Domenekunnskap** om b√•de botanikk og medisin\n",
    "- **Praktisk kode** som kan kj√∏res direkte\n",
    "\n",
    "Notebook-en er designet for √• v√¶re b√•de l√¶rerik og praktisk, med rikelig med forklarende tekst og sammenligninger med medisinsk bildeanalyse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-helse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
