{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/arvidl/AI-og-helse/blob/main/uke03-dyplæring/04a_ansiktsutrykk_klassifikasjon.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 😊 CNN for Ansiktsutrykk-klassifikasjon: Fra Emosjoner til Medisin\n",
    "\n",
    "Denne notebook demonstrerer hvordan **Konvolusjonelle Nevrale Nettverk (CNN)** kan brukes til å klassifisere ansiktsutrykk og emosjoner, og sammenligner dette med medisinske anvendelser.\n",
    "\n",
    "## Mål\n",
    "- Bygge og trene en CNN for å klassifisere 6 universelle emosjoner\n",
    "- Forstå hvordan CNN fungerer på ansiktsbilder\n",
    "- Sammenligne med medisinske anvendelser (depresjon, smerte, nevrologiske tilstander)\n",
    "- Demonstrere evalueringsmetoder: forvirringsmatrise, CAM/Grad-CAM\n",
    "- Diskutere etiske aspekter (bias, personvern)\n",
    "- Illustrere formalismen **y ~ f(X, θ)** i praksis\n",
    "\n",
    "## Datasett\n",
    "Vi bruker **FER2013** (Facial Expression Recognition 2013) datasettet med <strike>7</strike> 6 emosjonsklasser:\n",
    "- **<strike>Anger** (Sinne)</strike> - **denne er tom i FER2013**\n",
    "- 🤢 **Disgust** (Avsky) \n",
    "- 😨 **Fear** (Frykt)\n",
    "- 😊 **Happy** (Glede)\n",
    "- 😢 **Sad** (Tristhet)\n",
    "- 😲 **Surprise** (Overraskelse)\n",
    "- 😐 **Neutral** (Nøytral)\n",
    "\n",
    "## Teoretisk Fundament\n",
    "\n",
    "### Formalismen y ~ f(X, θ)\n",
    "\n",
    "I maskinlæring kan vi uttrykke emosjonsklassifikasjonsproblemet som:\n",
    "\n",
    "**y = f(X, θ) + ε**\n",
    "\n",
    "Hvor:\n",
    "- **y** = predikert emosjon (0-5)\n",
    "- **X** = input ansiktsbilde (pikselverdier)\n",
    "- **θ** = modellparametere (CNN-vekter)\n",
    "- **f** = ikke-lineær funksjon (CNN-arkitekturen)\n",
    "- **ε** = feilterm (noise)\n",
    "\n",
    "Dette er identisk med medisinsk bildeanalyse, bare med forskjellige klasser!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sammenligning: Emosjoner vs Medisinske Bilder\n",
    "\n",
    "| Aspekt | Emosjonsgjenkjenning | Medisinsk Bildeanalyse |\n",
    "|--------|---------------------|------------------------|\n",
    "| **Input (X)** | RGB/gråtoner av ansikter | Multiparametrisk MRI, røntgen, CT |\n",
    "| **Klasser (y)** | 6 universelle emosjoner | Sykdomstilstander, anatomiske strukturer |\n",
    "| **Kompleksitet** | Mikro-uttrykk, kulturelle forskjeller | Anatomiske strukturer, patologier |\n",
    "| **Konsekvenser** | Psykologisk vurdering | Frisk - Syk |\n",
    "| **Datamengde** | Tusener av bilder | Begrenset (privacy, ekspertise) |\n",
    "| **Ekspertise** | Psykologi, nevrologi | Medisin, radiologi |\n",
    "| **Bias** | Kulturell, etnisk, kjønnsbias | Demografisk, teknisk bias |\n",
    "\n",
    "**Felles prinsipper:**\n",
    "- Begge krever domenekunnskap\n",
    "- Begge har problemer med ubalanserte klasser  \n",
    "- Begge trenger robuste modeller\n",
    "- Begge har etiske implikasjoner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Men først: 🔧 miljøoppsett - kode skal fungere både lokalt, i Codespaces samt Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💻 Kjører i lokal miljø/Codespaces\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Sjekk om vi kjører i Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"🚀 Kjører i Google Colab\")\n",
    "else:\n",
    "    print(\"💻 Kjører i lokal miljø/Codespaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Gå til root-mappen\n",
    "    os.chdir('/content')\n",
    "    \n",
    "    # Sjekk nåværende mappe\n",
    "    print(f\"Nåværende mappe: {os.getcwd()}\")\n",
    "    \n",
    "    # Sjekk om mappen allerede eksisterer\n",
    "    if os.path.exists('AI-og-helse'):\n",
    "        print(\"✅ AI-og-helse mappen eksisterer allerede!\")\n",
    "        \n",
    "        # Sjekk innholdet\n",
    "        print(\"\\n📁 Innhold i AI-og-helse mappen:\")\n",
    "        try:\n",
    "            result = subprocess.run(['ls', '-la', 'AI-og-helse'], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(result.stdout)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Kunne ikke liste innhold: {e}\")\n",
    "        \n",
    "        # Sjekk om det er en git repository\n",
    "        if os.path.exists('AI-og-helse/.git'):\n",
    "            print(\"\\n✅ Dette er en git repository!\")\n",
    "            \n",
    "            # Gå inn i mappen og oppdater\n",
    "            os.chdir('AI-og-helse')\n",
    "            print(f\"📁 Byttet til: {os.getcwd()}\")\n",
    "            \n",
    "            # Prøv å oppdatere repositoryet\n",
    "            try:\n",
    "                result = subprocess.run(['git', 'pull'], \n",
    "                                      capture_output=True, text=True, check=True)\n",
    "                print(\"✅ Repository oppdatert!\")\n",
    "                print(result.stdout)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"⚠️ Kunne ikke oppdatere repository: {e}\")\n",
    "                print(\"Men mappen eksisterer og kan brukes!\")\n",
    "        else:\n",
    "            print(\"⚠️ Dette ser ikke ut som en git repository\")\n",
    "            \n",
    "    else:\n",
    "        print(\"�� Mappen eksisterer ikke - prøver git clone...\")\n",
    "        try:\n",
    "            result = subprocess.run(['git', 'clone', 'https://github.com/arvidl/AI-og-helse.git'], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(\"✅ Repository klonet vellykket!\")\n",
    "            print(result.stdout)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"❌ Git clone feilet: {e}\")\n",
    "            print(f\"Error output: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install opencv-python --quiet\n",
    "    !pip install tqdm --quiet\n",
    "    !pip install torchsummary --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Apple Silicon MPS tilgjengelig\n",
      "Bruker enhet: mps\n"
     ]
    }
   ],
   "source": [
    "# Imports og setup med feilhåndtering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sjekk om GPU eller MPS er tilgjengelig og sett enhet (device)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'🚀 GPU tilgjengelig: {torch.cuda.get_device_name(0)}')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(' Apple Silicon MPS tilgjengelig')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('💻 Bruker CPU')\n",
    "\n",
    "print(f'Bruker enhet: {device}')\n",
    "\n",
    "# Sett random seeds for reproduserbarhet\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Lasting og Forberedelse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FER2013 Datasett\n",
    "\n",
    "**FER2013** (Facial Expression Recognition 2013) er et av de mest brukte datasettene for emosjonsgjenkjenning:\n",
    "\n",
    "- **Størrelse**: 35,887 bilder\n",
    "- **Oppløsning**: 48x48 piksler\n",
    "- **Format**: Gråtoner\n",
    "- **Klasser**: 6 emosjoner (0-5)  [Oprinnelig 7 universelle emosjoner: \"Anger\" (sinne) mangler i FER2013]\n",
    "- **Split**: Training (28,709), PublicTest (3,589), PrivateTest (3,589)\n",
    "\n",
    "### Datasettstruktur\n",
    "```\n",
    "../data/\n",
    "└── ansiktsuttrykk/\n",
    "    ├── FER2013/\n",
    "    │   ├── train/\n",
    "    │   │   ├── 0_disgust/\n",
    "    │   │   ├── 1_fear/\n",
    "    │   │   ├── 2_happy/\n",
    "    │   │   ├── 3_sad/\n",
    "    │   │   ├── 4_surprise/\n",
    "    │   │   └── 5_neutral/\n",
    "    │   ├── val/\n",
    "    │   │   ├── 0_disgust/\n",
    "    │   │   ├── 1_fear/\n",
    "    │   │   ├── 2_happy/\n",
    "    │   │   ├── 3_sad/\n",
    "    │   │   ├── 4_surprise/\n",
    "    │   │   └── 5_neutral/\n",
    "    │   └── test/\n",
    "    │   │   ├── 0_disgust/\n",
    "    │   │   ├── 1_fear/\n",
    "    │   │   ├── 2_happy/\n",
    "    │   │   ├── 3_sad/\n",
    "    │   │   ├── 4_surprise/\n",
    "    │   │   └── 5_neutral/\n",
    "    └── fer2013.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hent_fer2013_dataset():\n",
    "    \"\"\"\n",
    "    Hent FER2013 datasettet fra Kaggle (hvis ikke allerede lastet ned)\n",
    "    \"\"\"\n",
    "    print(\"📥 Henter FER2013 datasett\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    data_dir = Path(\"../data/ansiktsuttrykk/FER2013\")\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Sjekk om data allerede eksisterer\n",
    "    existing_images = list(data_dir.glob(\"**/*.png\")) + list(data_dir.glob(\"**/*.jpg\"))\n",
    "    \n",
    "    if existing_images:\n",
    "        print(f\"✅ Data allerede eksisterer: {len(existing_images)} bilder\")\n",
    "        return True\n",
    "    \n",
    "    # Prøv å laste ned fra Kaggle\n",
    "    try:\n",
    "        if IN_COLAB:\n",
    "            # I Colab, last ned fra Kaggle\n",
    "            !pip install kaggle --quiet\n",
    "            # Used to securely store your API key\n",
    "            from google.colab import userdata\n",
    "\n",
    "            # Load Kaggle credentials from Colab Secrets\n",
    "            try:\n",
    "                os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
    "                os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
    "                print(\"✅ Kaggle credentials loaded from Colab Secrets!\")\n",
    "            except userdata.notebook_secret.NotebookAccessError:\n",
    "                print(\"❌ Could not load Kaggle credentials from Colab Secrets.\")\n",
    "                print(\"Please ensure you have added KAGGLE_USERNAME and KAGGLE_KEY to the Secrets manager (🔑 icon) and enabled 'Notebook access'.\")\n",
    "                os.environ['KAGGLE_USERNAME'] = '' # Clear environment variables if access failed\n",
    "                os.environ['KAGGLE_KEY'] = ''\n",
    "\n",
    "            # Re-run the data download cell\n",
    "            !kaggle datasets download -d msambare/fer2013 -p {data_dir} --unzip\n",
    "        else:\n",
    "            # Lokalt, sjekk om Kaggle API er tilgjengelig\n",
    "            kaggle_path = Path.home() / \".kaggle\" / \"kaggle.json\"\n",
    "            if kaggle_path.exists():\n",
    "                print(\"✅ Kaggle API credentials funnet!\")\n",
    "                print(\"🔄 Prøver automatisk nedlasting...\")\n",
    "                \n",
    "                # Last ned datasettet\n",
    "                import subprocess\n",
    "                result = subprocess.run([\n",
    "                    \"kaggle\", \"datasets\", \"download\", \n",
    "                    \"-d\", \"msambare/fer2013\", \n",
    "                    \"-p\", str(data_dir), \"--unzip\"\n",
    "                ], capture_output=True, text=True)\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    print(\"✅ Datasett lastet ned!\")\n",
    "                else:\n",
    "                    print(f\"❌ Nedlasting feilet: {result.stderr}\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(\"⚠️ Kaggle API credentials ikke funnet!\")\n",
    "                print(\"Gå til: https://www.kaggle.com/datasets/msambare/fer2013\")\n",
    "                print(f\"Last ned og ekstraher til: {data_dir}\")\n",
    "                return False\n",
    "        \n",
    "        # Sjekk om nedlasting var vellykket\n",
    "        downloaded_images = list(data_dir.glob(\"**/*.png\")) + list(data_dir.glob(\"**/*.jpg\"))\n",
    "        if downloaded_images:\n",
    "            print(f\"✅ Nedlasting vellykket: {len(downloaded_images)} bilder\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Ingen bilder funnet etter nedlasting!\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Feil under nedlasting: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organiser_fer2013_dataset():\n",
    "    \"\"\"\n",
    "    Organiser FER2013 datasettet i train/val/test mapper\n",
    "    \"\"\"\n",
    "    print(\"🔧 Organiserer FER2013 datasett\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    data_dir = Path(\"../data/ansiktsuttrykk/FER2013\")\n",
    "    \n",
    "    # Emosjonsklasser\n",
    "    emotion_classes = {\n",
    "        0: 'disgust', \n",
    "        1: 'fear',\n",
    "        2: 'happy',\n",
    "        3: 'sad',\n",
    "        4: 'surprise',\n",
    "        5: 'neutral'\n",
    "    }\n",
    "    \n",
    "    # Opprett undermapper for alle splits og emosjonsklasser\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for emotion in emotion_classes.values():\n",
    "            (data_dir / split / emotion).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Sjekk om data allerede er organisert\n",
    "    val_images = list((data_dir / \"val\").glob(\"**/*.png\")) + list((data_dir / \"val\").glob(\"**/*.jpg\"))\n",
    "    \n",
    "    if val_images:\n",
    "        print(\"✅ Data allerede organisert!\")\n",
    "        return True\n",
    "    \n",
    "    # Hvis val-mappen er tom, flytt bilder fra train\n",
    "    print(\"🔄 Val-mappen er tom - flytter bilder fra train...\")\n",
    "    \n",
    "    total_moved = 0\n",
    "    \n",
    "    # Gå gjennom hver emosjonsklasse\n",
    "    for emotion in emotion_classes.values():\n",
    "        train_emotion_dir = data_dir / \"train\" / emotion\n",
    "        val_emotion_dir = data_dir / \"val\" / emotion\n",
    "        \n",
    "        # Finn alle bilder i denne emosjonsklassen\n",
    "        emotion_images = list(train_emotion_dir.glob(\"*.png\")) + list(train_emotion_dir.glob(\"*.jpg\"))\n",
    "        \n",
    "        if not emotion_images:\n",
    "            print(f\"  ⚠️ {emotion}: Ingen bilder funnet i train\")\n",
    "            continue\n",
    "            \n",
    "        # Flytt 15% av bildene til val\n",
    "        val_count = int(len(emotion_images) * 0.15)\n",
    "        \n",
    "        if val_count > 0:\n",
    "            print(f\"  {emotion}: {len(emotion_images)} bilder → flytter {val_count} til val\")\n",
    "            \n",
    "            # Flytt bildene\n",
    "            moved_count = 0\n",
    "            for image_path in emotion_images:\n",
    "                if moved_count >= val_count:\n",
    "                    break\n",
    "                    \n",
    "                # Opprett ny sti i val-mappen\n",
    "                val_path = val_emotion_dir / image_path.name\n",
    "                \n",
    "                # Flytt bildet\n",
    "                try:\n",
    "                    image_path.rename(val_path)\n",
    "                    moved_count += 1\n",
    "                    total_moved += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Kunne ikke flytte {image_path.name}: {e}\")\n",
    "        else:\n",
    "            print(f\"  {emotion}: {len(emotion_images)} bilder → ingen flyttet (for få bilder)\")\n",
    "    \n",
    "    print(f\"✅ Flyttet totalt {total_moved} bilder til val!\")\n",
    "    \n",
    "    # Vis statistikk\n",
    "    print(\"\\n�� Ny organisering:\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_images = list((data_dir / split).glob(\"**/*.png\")) + list((data_dir / split).glob(\"**/*.jpg\"))\n",
    "        print(f\"  {split}: {len(split_images)} bilder\")\n",
    "        \n",
    "        # Vis per emosjonsklasse\n",
    "        for emotion in emotion_classes.values():\n",
    "            emotion_images = list((data_dir / split / emotion).glob(\"*.png\")) + list((data_dir / split / emotion).glob(\"*.jpg\"))\n",
    "            if emotion_images:\n",
    "                print(f\"    {emotion}: {len(emotion_images)} bilder\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Henter FER2013 datasett\n",
      "==================================================\n",
      "✅ Data allerede eksisterer: 35887 bilder\n",
      "CPU times: user 106 ms, sys: 60 ms, total: 166 ms\n",
      "Wall time: 185 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Hent data (hvis ikke allerede lastet ned)\n",
    "success = hent_fer2013_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Organiserer FER2013 datasett\n",
      "==================================================\n",
      "✅ Data allerede organisert!\n"
     ]
    }
   ],
   "source": [
    "# Organiser data (hvis ikke allerede organisert)\n",
    "if success:\n",
    "    organiser_fer2013_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FER2013 data eksisterer: 35887 bilder\n",
      "\n",
      "TRAIN:\n",
      "  disgust: 371 bilder\n",
      "  fear: 3483 bilder\n",
      "  happy: 6133 bilder\n",
      "  sad: 4106 bilder\n",
      "  surprise: 2696 bilder\n",
      "  neutral: 4221 bilder\n",
      "\n",
      "VAL:\n",
      "  disgust: 65 bilder\n",
      "  fear: 614 bilder\n",
      "  happy: 1082 bilder\n",
      "  sad: 724 bilder\n",
      "  surprise: 475 bilder\n",
      "  neutral: 744 bilder\n",
      "\n",
      "TEST:\n",
      "  disgust: 111 bilder\n",
      "  fear: 1024 bilder\n",
      "  happy: 1774 bilder\n",
      "  sad: 1247 bilder\n",
      "  surprise: 831 bilder\n",
      "  neutral: 1233 bilder\n"
     ]
    }
   ],
   "source": [
    "def check_fer2013_data():\n",
    "    \"\"\"Sjekk om FER2013 data eksisterer og er organisert\"\"\"\n",
    "    data_dir = Path(\"../data/ansiktsuttrykk/FER2013\")\n",
    "    \n",
    "    # Sjekk om bilder eksisterer\n",
    "    existing_images = list(data_dir.glob(\"**/*.jpg\")) + list(data_dir.glob(\"**/*.png\"))\n",
    "    \n",
    "    if existing_images:\n",
    "        print(f\"✅ FER2013 data eksisterer: {len(existing_images)} bilder\")\n",
    "        \n",
    "        # Tell bilder per split og emosjon\n",
    "        splits = ['train', 'val', 'test']\n",
    "        emotions = ['anger', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "        \n",
    "        for split in splits:\n",
    "            print(f\"\\n{split.upper()}:\")\n",
    "            for emotion in emotions:\n",
    "                count = len(list((data_dir / split / emotion).glob(\"*.jpg\")) + \n",
    "                           list((data_dir / split / emotion).glob(\"*.png\")))\n",
    "                if count > 0:\n",
    "                    print(f\"  {emotion}: {count} bilder\")\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ Ingen FER2013 data funnet\")\n",
    "        return False\n",
    "\n",
    "# Kjør sjekk\n",
    "data_ready = check_fer2013_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    \"\"\"Custom dataset for emotion classification\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('L')  # Gråtoner\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emotion_data(data_dir, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"Last og splitt emosjonsdatasett\"\"\"\n",
    "    \n",
    "    # Emosjonsklasser\n",
    "    emotion_classes = {\n",
    "        0: 'disgust', \n",
    "        1: 'fear',\n",
    "        2: 'happy',\n",
    "        3: 'sad',\n",
    "        4: 'surprise',\n",
    "        5: 'neutral'\n",
    "    }\n",
    "    \n",
    "    # Samle alle bilde-sti og etiketter\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_dir = os.path.join(data_dir, split)\n",
    "        if os.path.exists(split_dir):\n",
    "            for emotion_name in emotion_classes.values():\n",
    "                emotion_dir = os.path.join(split_dir, emotion_name)\n",
    "                if os.path.exists(emotion_dir):\n",
    "                    for filename in os.listdir(emotion_dir):\n",
    "                        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            image_paths.append(os.path.join(emotion_dir, filename))\n",
    "                            # Finn emosjonsindeks\n",
    "                            emotion_idx = [k for k, v in emotion_classes.items() if v == emotion_name][0]\n",
    "                            labels.append(emotion_idx)\n",
    "    \n",
    "    print(f\"Totalt bilder funnet: {len(image_paths)}\")\n",
    "    print(f\"Emosjoner: {list(emotion_classes.values())}\")\n",
    "    print(f\"Bilder per emosjon: {np.bincount(labels)}\")\n",
    "    \n",
    "    # Splitt data hvis nødvendig\n",
    "    if len(set(labels)) > 1:  # Sjekk om vi har flere klasser\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            image_paths, labels, test_size=test_size + val_size, \n",
    "            random_state=42, stratify=labels\n",
    "        )\n",
    "        \n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_temp, y_temp, test_size=test_size/(test_size + val_size),\n",
    "            random_state=42, stratify=y_temp\n",
    "        )\n",
    "    else:\n",
    "        # Fallback hvis vi bare har én klasse\n",
    "        X_train, X_val, X_test = image_paths, image_paths, image_paths\n",
    "        y_train, y_val, y_test = labels, labels, labels\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), list(emotion_classes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data transformasjoner definert!\n"
     ]
    }
   ],
   "source": [
    "# Data transformasjoner\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normaliser gråtoner\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "print(\"✅ Data transformasjoner definert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN Modell for Emosjonsgjenkjenning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionNet(nn.Module):\n",
    "    \"\"\"CNN for emosjonsklassifikasjon\"\"\"\n",
    "    \n",
    "    emotion_classes = {\n",
    "        0: 'disgust', \n",
    "        1: 'fear',\n",
    "        2: 'happy',\n",
    "        3: 'sad',\n",
    "        4: 'surprise',\n",
    "        5: 'neutral'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, num_classes=len(emotion_classes), dropout_rate=0.5):\n",
    "        super(EmotionNet, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Block 2  \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.25),\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling for ulike input-størrelser\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((3, 3))\n",
    "        \n",
    "        # Klassifikator\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 3 * 3, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forklaring av EmotionNet arkitekturen\n",
    "\n",
    "**EmotionNet** er spesielt designet for emosjonsgjenkjenning med følgende egenskaper:\n",
    "\n",
    "#### **1. Input Layer**\n",
    "- **1 kanal**: Gråtoner (FER2013 er gråtoner)\n",
    "- **48x48 piksler**: Standard FER2013 oppløsning\n",
    "\n",
    "#### **2. Feature Extraction (4 blokker)**\n",
    "```python\n",
    "# Block 1: Grunnleggende kanter og teksturer\n",
    "Conv2d(1, 32, 3x3) → BatchNorm → ReLU → MaxPool(2x2) → Dropout2d(0.25)\n",
    "\n",
    "# Block 2: Mer komplekse mønstre\n",
    "Conv2d(32, 64, 3x3) → BatchNorm → ReLU → MaxPool(2x2) → Dropout2d(0.25)\n",
    "\n",
    "# Block 3: Høyere nivå features\n",
    "Conv2d(64, 128, 3x3) → BatchNorm → ReLU → MaxPool(2x2) → Dropout2d(0.25)\n",
    "\n",
    "# Block 4: Komplekse emosjonelle features\n",
    "Conv2d(128, 256, 3x3) → BatchNorm → ReLU → MaxPool(2x2) → Dropout2d(0.25)\n",
    "```\n",
    "\n",
    "#### **3. Regularisering**\n",
    "- **BatchNorm2d**: Stabiliserer trening\n",
    "- **Dropout2d**: Forhindrer overfitting\n",
    "- **Dropout**: I fully-connected layers\n",
    "\n",
    "#### **4. Klassifikator**\n",
    "- **AdaptiveAvgPool2d(3x3)**: Reduserer til 3x3 spatial dimensjoner\n",
    "- **3 FC layers**: 256×9 → 512 → 256 → 6\n",
    "- **6 output klasser**: 6 emosjoner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treningsfunksjoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Tren model for én epoke\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Valider model for én epoke\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "    \"\"\"Tren den fullstendige modellen\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Starter trening...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoke {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), './modeller/best_emotion_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= 10:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses, \n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'best_val_acc': best_val_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hovedkjøring - Del 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hovedkjøring med automatisk oppsett\n",
    "def main_part1():\n",
    "    \"\"\"Hovedkjøring del 1: Data lasting og modell oppsett\"\"\"\n",
    "    \n",
    "    print(\" Ansiktsutrykk-klassifikasjon med CNN - Del 1\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sett opp data-mapper\n",
    "    data_dir = \"../data/ansiktsuttrykk/FER2013\"\n",
    "    \n",
    "    # Sjekk om data eksisterer\n",
    "    if not os.path.exists(data_dir) or len(os.listdir(data_dir)) == 0:\n",
    "        print(\"❌ Data directory not found or empty!\")\n",
    "        print(\"\\n🔄 Prøver automatisk oppsett...\")\n",
    "        \n",
    "        # Prøv automatisk nedlasting\n",
    "        success = download_fer2013_dataset()\n",
    "        \n",
    "        if not success:\n",
    "            print(\"\\n❌ Automatisk oppsett feilet!\")\n",
    "            print(\"\\n📋 Manuell nedlasting:\")\n",
    "            print(\"1. Gå til: https://www.kaggle.com/datasets/msambare/fer2013\")\n",
    "            print(\"2. Last ned 'fer2013.zip'\")\n",
    "            print(\"3. Ekstraher til '../data/ansiktsuttrykk/' mappen\")\n",
    "            return None, None, None, None, None\n",
    "    else:\n",
    "        print(\"✅ Data-katalog funnet!\")\n",
    "    \n",
    "    # Last data\n",
    "    print(\"\\n Laster data...\")\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test), class_names = load_emotion_data(data_dir)\n",
    "    \n",
    "    # Etabler datasett\n",
    "    print(\"\\n🔧 Etablerer trening-, validering- og test-datasett...\")\n",
    "    train_dataset = EmotionDataset(X_train, y_train, train_transform)\n",
    "    val_dataset = EmotionDataset(X_val, y_val, val_transform)\n",
    "    test_dataset = EmotionDataset(X_test, y_test, val_transform)\n",
    "    \n",
    "    # Etabler data-laster \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"✅ Lasting av data vellykket!\")\n",
    "    print(f\"Trening: {len(train_dataset)} bilder\")\n",
    "    print(f\"Validering: {len(val_dataset)} bilder\") \n",
    "    print(f\"Test: {len(test_dataset)} bilder\")\n",
    "    \n",
    "    # Bygg modell\n",
    "    print(\"\\n��️ Bygger modell...\")\n",
    "    model = EmotionNet(num_classes=len(class_names)).to(device)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    return model, train_loader, val_loader, test_loader, class_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ansiktsutrykk-klassifikasjon med CNN - Del 1\n",
      "============================================================\n",
      "✅ Data-katalog funnet!\n",
      "\n",
      " Laster data...\n",
      "Totalt bilder funnet: 30934\n",
      "Emosjoner: ['disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
      "Bilder per emosjon: [ 547 5121 8989 6077 4002 6198]\n",
      "\n",
      "🔧 Etablerer trening-, validering- og test-datasett...\n",
      "✅ Lasting av data vellykket!\n",
      "Trening: 18560 bilder\n",
      "Validering: 6187 bilder\n",
      "Test: 6187 bilder\n",
      "\n",
      "��️ Bygger modell...\n",
      "Model parameters: 1,701,830\n"
     ]
    }
   ],
   "source": [
    "# Kjør hovedkjøring\n",
    "model, train_loader, val_loader, test_loader, class_names = main_part1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Modell Sammendrag:\n",
      "==================================================\n",
      "🧪 Tester modell med dummy input...\n",
      "✅ Modell test vellykket!\n",
      "Input shape: torch.Size([1, 1, 48, 48])\n",
      "Output shape: torch.Size([1, 6])\n",
      "Output device: mps:0\n",
      "\n",
      "📋 Modell Detaljer:\n",
      "Modell: EmotionNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout2d(p=0.25, inplace=False)\n",
      "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Dropout2d(p=0.25, inplace=False)\n",
      "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Dropout2d(p=0.25, inplace=False)\n",
      "    (15): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Dropout2d(p=0.25, inplace=False)\n",
      "  )\n",
      "  (adaptive_pool): AdaptiveAvgPool2d(output_size=(3, 3))\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=2304, out_features=512, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=256, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "Enhet: mps\n",
      "Input størrelse: (batch_size, 1, 48, 48)\n",
      "Output størrelse: (batch_size, 6)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 48, 48]             320\n",
      "       BatchNorm2d-2           [-1, 32, 48, 48]              64\n",
      "              ReLU-3           [-1, 32, 48, 48]               0\n",
      "         MaxPool2d-4           [-1, 32, 24, 24]               0\n",
      "         Dropout2d-5           [-1, 32, 24, 24]               0\n",
      "            Conv2d-6           [-1, 64, 24, 24]          18,496\n",
      "       BatchNorm2d-7           [-1, 64, 24, 24]             128\n",
      "              ReLU-8           [-1, 64, 24, 24]               0\n",
      "         MaxPool2d-9           [-1, 64, 12, 12]               0\n",
      "        Dropout2d-10           [-1, 64, 12, 12]               0\n",
      "           Conv2d-11          [-1, 128, 12, 12]          73,856\n",
      "      BatchNorm2d-12          [-1, 128, 12, 12]             256\n",
      "             ReLU-13          [-1, 128, 12, 12]               0\n",
      "        MaxPool2d-14            [-1, 128, 6, 6]               0\n",
      "        Dropout2d-15            [-1, 128, 6, 6]               0\n",
      "           Conv2d-16            [-1, 256, 6, 6]         295,168\n",
      "      BatchNorm2d-17            [-1, 256, 6, 6]             512\n",
      "             ReLU-18            [-1, 256, 6, 6]               0\n",
      "        MaxPool2d-19            [-1, 256, 3, 3]               0\n",
      "        Dropout2d-20            [-1, 256, 3, 3]               0\n",
      "AdaptiveAvgPool2d-21            [-1, 256, 3, 3]               0\n",
      "          Flatten-22                 [-1, 2304]               0\n",
      "           Linear-23                  [-1, 512]       1,180,160\n",
      "             ReLU-24                  [-1, 512]               0\n",
      "          Dropout-25                  [-1, 512]               0\n",
      "           Linear-26                  [-1, 256]         131,328\n",
      "             ReLU-27                  [-1, 256]               0\n",
      "          Dropout-28                  [-1, 256]               0\n",
      "           Linear-29                    [-1, 6]           1,542\n",
      "================================================================\n",
      "Total params: 1,701,830\n",
      "Trainable params: 1,701,830\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.74\n",
      "Params size (MB): 6.49\n",
      "Estimated Total Size (MB): 10.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Vis modellstruktur\n",
    "print(\"\\n📊 Modell Sammendrag:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test modellen med dummy input først\n",
    "print(\"🧪 Tester modell med dummy input...\")\n",
    "try:\n",
    "    dummy_input = torch.randn(1, 1, 48, 48).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "    print(f\"✅ Modell test vellykket!\")\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output device: {output.device}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Modell test feilet: {e}\")\n",
    "\n",
    "# Vis modellstruktur manuelt\n",
    "print(f\"\\n📋 Modell Detaljer:\")\n",
    "print(f\"Modell: {model}\")\n",
    "print(f\"Enhet: {device}\")\n",
    "print(f\"Input størrelse: (batch_size, 1, 48, 48)\")\n",
    "print(f\"Output størrelse: (batch_size, {len(class_names)})\")\n",
    "\n",
    "# Prøv torchsummary hvis modellen fungerer\n",
    "try:\n",
    "    from torchsummary import summary\n",
    "    # Sørg for at modellen er på CPU for torchsummary\n",
    "    model_cpu = model.cpu()\n",
    "    summary(model_cpu, input_size=(1, 48, 48))  # (kanaler, høyde, bredde)\n",
    "    # Flytt modellen tilbake til riktig enhet\n",
    "    model = model.to(device)\n",
    "except ImportError:\n",
    "    print(\"\\n⚠️ torchsummary ikke tilgjengelig - installer med: pip install torchsummary\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ torchsummary feilet: {e}\")\n",
    "    print(\"Bruker manuell modellvisning i stedet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Oppsummering av Del 1\n",
    "\n",
    "**Del 1** av notebooken har nå:\n",
    "\n",
    "✅ **Miljøoppsett** - Fungerer på Colab, Codespaces og lokalt<br>\n",
    "✅ **Device detection** - Automatisk GPU/MPS/CPU valg<br>\n",
    "✅ **Data lasting** - FER2013 datasett med 7 emosjonsklasser<br>\n",
    "✅ **Modell definisjon** - EmotionNet CNN arkitektur<br>\n",
    "✅ **Treningsfunksjoner** - Komplette trenings- og valideringsfunksjoner<br>\n",
    "✅ **Hovedkjøring** - Automatisk oppsett og modell initialisering<br>\n",
    "\n",
    "**Neste steg** (Del 2) vil inkludere:\n",
    "- Visuell inspeksjon av data\n",
    "- Modell trening\n",
    "- Evaluering og visualisering\n",
    "- CAM/Grad-CAM for forklarbar AI\n",
    "- Medisinske anvendelser og etiske diskusjoner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-helse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
