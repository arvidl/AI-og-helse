{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Medisinsk Bildeanalyse med MR og PyTorch\n",
    "\n",
    "I denne notebook-en skal vi bygge en dyplæringsmodell for å analysere medisinske bilder. Vi går bort fra røntgenbildene vi så på tidligere, og fokuserer nå på **magnetisk resonanstomografi (MR)**-bilder av hjernen.\n",
    "\n",
    "**Mål:** Vi skal trene et **Convolutional Neural Network (CNN)** til å klassifisere MR-bilder og skille mellom hjerner fra friske kontrollpersoner og personer med demens.\n",
    "\n",
    "**Datasett:** Vi bruker et lite utvalg fra [OASIS-1](https://www.oasis-brains.org/)-datasettet. Dataene består av 3D MR-bilder i NIfTI-format (`.nii`). For å gjøre oppgaven enklere, vil vi trekke ut ett 2D-snitt fra midten av hver 3D-skanning for å utføre 2D-bildeklassifisering.\n",
    "\n",
    "**Verktøy:**\n",
    "- **PyTorch:** Et populært rammeverk for dyp læring.\n",
    "- **Nibabel:** Et Python-bibliotek for å lese og skrive vanlige medisinske bildeformater, som NIfTI.\n",
    "- **Scikit-learn:** For datasplitting og evaluering.\n",
    "- **Matplotlib:** For visualisering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steg 1: Importere Nødvendige Biblioteker\n",
    "\n",
    "Først importerer vi alle bibliotekene vi trenger for databehandling, modellbygging, trening og evaluering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nibabel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnibabel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnib\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nibabel'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Sjekk om GPU er tilgjengelig og sett enhet (device)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Bruker enhet: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steg 2: Laste inn og Utforske Dataene\n",
    "\n",
    "Vi antar at dataene er organisert i en mappestruktur som dette:\n",
    "```\n",
    "data/\n",
    "└── oasis_mri_sample/\n",
    "    ├── demented/\n",
    "    │   └── subject_1.nii\n",
    "    └── nondemented/\n",
    "        └── subject_2.nii\n",
    "```\n",
    "\n",
    "Vi lager en funksjon som leser alle `.nii`-filene, trekker ut det midterste 2D-snittet fra 3D-volumet, og lagrer bildene sammen med sine etiketter (0 for `nondemented`, 1 for `demented`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mri_data(data_path):\n",
    "    \"\"\"Laster inn MR-data og returnerer 2D-snitt og etiketter.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for label, category in enumerate(['nondemented', 'demented']):\n",
    "        category_path = os.path.join(data_path, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            print(f\"Advarsel: Mappen {category_path} ble ikke funnet.\")\n",
    "            continue\n",
    "            \n",
    "        for filename in os.listdir(category_path):\n",
    "            if filename.endswith('.nii'):\n",
    "                img_path = os.path.join(category_path, filename)\n",
    "                try:\n",
    "                    # Last inn NIfTI-filen\n",
    "                    nii_img = nib.load(img_path)\n",
    "                    img_data = nii_img.get_fdata()\n",
    "                    \n",
    "                    # Hent ut det midterste snittet langs z-aksen\n",
    "                    mid_slice_idx = img_data.shape[2] // 2\n",
    "                    mid_slice = img_data[:, :, mid_slice_idx]\n",
    "                    \n",
    "                    # Roter bildet for riktig visning\n",
    "                    mid_slice = np.rot90(mid_slice)\n",
    "                    \n",
    "                    images.append(mid_slice)\n",
    "                    labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Kunne ikke laste {img_path}: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Angi stien til dataene (endre denne om nødvendig)\n",
    "DATA_DIR = '../data/oasis_mri_sample/' # Antar at dataene ligger i en mappe på samme nivå som `uke03`\n",
    "images, labels = load_mri_data(DATA_DIR)\n",
    "\n",
    "if len(images) > 0:\n",
    "    print(f'Lastet inn {len(images)} bilder.')\n",
    "    print(f'Bildedimensjoner: {images[0].shape}')\n",
    "    print(f'Antall i hver klasse: {np.bincount(labels)}')\n",
    "else:\n",
    "    print('Ingen bilder ble lastet inn. Sjekk stien til dataene og mappestrukturen.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisere Eksempelbilder\n",
    "\n",
    "Det er alltid lurt å se på dataene for å få en intuisjon for hva modellen skal lære. Vi viser ett eksempel fra hver klasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(images) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Finn første bilde av en frisk person\n",
    "    nondemented_idx = np.where(labels == 0)[0][0]\n",
    "    axes[0].imshow(images[nondemented_idx], cmap='gray')\n",
    "    axes[0].set_title('Klasse: Nondemented (Frisk)')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Finn første bilde av en dement person\n",
    "    demented_idx = np.where(labels == 1)[0][0]\n",
    "    axes[1].imshow(images[demented_idx], cmap='gray')\n",
    "    axes[1].set_title('Klasse: Demented (Demens)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steg 3: Forprosessering og Klargjøring av Data\n",
    "\n",
    "For at et nevralt nettverk skal kunne behandle bildene, må vi:\n",
    "1.  **Dele dataene** inn i et treningssett og et valideringssett.\n",
    "2.  **Opprette en egendefinert `Dataset`-klasse** i PyTorch. Dette er standard praksis for å håndtere data effektivt.\n",
    "3.  **Definere transformasjoner:** Bildene må konverteres til PyTorch-tensorer, få endret størrelse til en fast dimensjon (f.eks. 128x128), og normaliseres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Del dataene i trenings- og valideringssett\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    images, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f'Størrelse på treningssett: {len(X_train)}')\n",
    "print(f'Størrelse på valideringssett: {len(X_val)}')\n",
    "\n",
    "# 2. Lag en egendefinert Dataset-klasse\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Legg til en kanal-dimensjon (for gråtonebilder)\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# 3. Definer transformasjoner\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    # Normalisering med gjennomsnitt og standardavvik for datasettet\n",
    "    # For enkelhets skyld bruker vi (0.5, 0.5) som er vanlig for bilder i [-1, 1] området\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Opprett Dataset- og DataLoader-objekter\n",
    "train_dataset = MRIDataset(X_train, y_train, transform=data_transforms)\n",
    "val_dataset = MRIDataset(X_val, y_val, transform=data_transforms)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steg 4: Bygge CNN-Modellen\n",
    "\n",
    "Nå definerer vi arkitekturen til vårt nevrale nettverk. Vi lager en enkel CNN-modell med to konvolusjonslag etterfulgt av to fullt-tilkoblede (dense) lag.\n",
    "\n",
    "**Arkitektur:**\n",
    "1.  `Conv2d` -> `ReLU` -> `MaxPool2d` (Første konvolusjonsblokk)\n",
    "2.  `Conv2d` -> `ReLU` -> `MaxPool2d` (Andre konvolusjonsblokk)\n",
    "3.  `Flatten` (Gjør om 2D-kart til 1D-vektor)\n",
    "4.  `Linear` -> `ReLU` (Første tette lag)\n",
    "5.  `Linear` (Output-lag, som gir en score for hver klasse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # Etter to pooling-lag vil et 128x128 bilde bli 32x32\n",
    "        # Størrelsen på input til det lineære laget er 32 * 32 * 32 (kanaler)\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 32 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2) # 2 klasser: nondemented og demented\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "# Initialiser modellen og flytt den til GPU hvis tilgjengelig\n",
    "model = SimpleCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steg 5: Trene Modellen\n",
    "\n",
    "Nå er vi klare til å trene modellen. Vi må definere:\n",
    "- **En tapsfunksjon (Loss Function):** Måler hvor feil modellen predikerer. `CrossEntropyLoss` er standard for klassifiseringsoppgaver.\n",
    "- **En optimaliseringsalgoritme (Optimizer):** Oppdaterer vektene i nettverket for å minimere tapet. `Adam` er et robust og populært valg.\n",
    "\n",
    "Deretter skriver vi en treningsløkke som itererer over dataene i et visst antall *epochs*. For hver epoch trener vi på treningssettet og evaluerer på valideringssettet for å overvåke ytelsen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definer hyperparametere\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 20\n",
    "\n",
    "# Definer tapsfunksjon og optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # --- Treningsfase ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Nullstill gradienter\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass og optimalisering\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = correct_train / total_train\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "\n",
    "    # --- Valideringsfase ---\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad(): # Ingen grunn til å beregne gradienter her\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} | ' \\\n",
    "          f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | ' \\\n",
    "          f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steg 6: Evaluere Modellen\n",
    "\n",
    "Etter treningen er det viktig å evaluere modellens ytelse. Vi gjør dette på flere måter:\n",
    "1.  **Plotte treningshistorikk:** Vi ser på hvordan tap og nøyaktighet utvikler seg over tid for både trenings- og valideringssettet. Dette kan avsløre problemer som *overfitting*.\n",
    "2.  **Forvirringsmatrise (Confusion Matrix):** Viser en tabell over hvor mange bilder som ble korrekt og feilaktig klassifisert for hver klasse. Dette gir et mer detaljert bilde enn bare nøyaktighet.\n",
    "3.  **Klassifiseringsrapport:** Gir en oppsummering av presisjon, sensitivitet (recall) og F1-score for hver klasse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Plotte treningshistorikk\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(history['train_loss'], label='Training Loss')\n",
    "ax1.plot(history['val_loss'], label='Validation Loss')\n",
    "ax1.set_title('Loss over Epochs')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history['train_acc'], label='Training Accuracy')\n",
    "ax2.plot(history['val_acc'], label='Validation Accuracy')\n",
    "ax2.set_title('Accuracy over Epochs')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 & 3. Forvirringsmatrise og klassifiseringsrapport\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "class_names = ['Nondemented', 'Demented']\n",
    "print(\"Klassifiseringsrapport:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Forvirringsmatrise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konklusjon\n",
    "\n",
    "I denne notebook-en har vi bygget en ende-til-ende-løsning for klassifisering av MR-bilder ved hjelp av PyTorch.\n",
    "\n",
    "Vi har:\n",
    "1.  Lastet inn og forprosessert medisinske bilder i NIfTI-format.\n",
    "2.  Definert en egendefinert `Dataset`-klasse for å håndtere dataene i PyTorch.\n",
    "3.  Bygget og trent en enkel CNN-modell fra bunnen av.\n",
    "4.  Evaluert modellens ytelse med relevante metrikker som nøyaktighet, forvirringsmatrise og klassifiseringsrapport.\n",
    "\n",
    "Dette er et startpunkt. For å forbedre modellen kan man utforske:\n",
    "- **Dataaugmentering:** Roter, zoom eller flipp bildene for å kunstig øke størrelsen på treningssettet.\n",
    "- **Mer avanserte arkitekturer:** Bruk forhåndstrente modeller som ResNet (transfer learning).\n",
    "- **3D-konvolusjoner:** Utnytt den fulle 3D-informasjonen i MR-skanningene ved å bruke `Conv3d`-lag."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-helse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
