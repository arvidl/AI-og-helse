{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9940420",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/arvidl/AI-og-helse/blob/main/uke04-generativ-ai/02_llm_grunnleggende.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5817709",
   "metadata": {},
   "source": [
    "# ü§ñ Store Spr√•kmodeller (LLM) - Grunnleggende konsepter\n",
    "\n",
    "## L√¶ringsm√•l\n",
    "- Forst√• hvordan LLM genererer tekst\n",
    "- L√¶re om tokens og embeddings\n",
    "- Utforske temperature og sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2d7ec",
   "metadata": {},
   "source": [
    "### Men f√∏rst: üîß milj√∏oppsett - kode skal fungere b√•de lokalt, i Codespaces samt Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b468e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Kj√∏rer i lokal milj√∏/Codespaces\n",
      "‚úÖ Milj√∏ er konfigurert og klart!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Sjekk om vi kj√∏rer i Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üöÄ Kj√∏rer i Google Colab\")\n",
    "    \n",
    "    # Installer n√∏dvendige pakker som ikke er forh√•ndsinstallert i Colab\n",
    "    !pip install seaborn --quiet\n",
    "    \n",
    "    # Sjekk om mappen allerede eksisterer\n",
    "    if not os.path.exists('AI-og-helse'):\n",
    "        print(\"üì• Laster ned kursmateriell...\")\n",
    "        try:\n",
    "            # Pr√∏v √• klone repositoryet (da v√¶re public)\n",
    "            !git clone https://github.com/arvidl/AI-og-helse.git\n",
    "            print(\"‚úÖ Repository klonet vellykket!\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Kunne ikke klone repository automatisk\")\n",
    "            print(\"üí° Du kan laste opp filer manuelt eller bruke en annen metode\")\n",
    "    \n",
    "    # Bytt til riktig mappe hvis den eksisterer\n",
    "    if os.path.exists('AI-og-helse'):\n",
    "        os.chdir('AI-og-helse')\n",
    "        print(f\"üìÅ Byttet til mappe: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(\"üìÇ Arbeider i standard Colab-mappe\")\n",
    "        \n",
    "else:\n",
    "    print(\"üíª Kj√∏rer i lokal milj√∏/Codespaces\")\n",
    "\n",
    "# Standard imports som fungerer overalt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ Milj√∏ er konfigurert og klart!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fad73b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LLM Grunnleggende - Fra tekst til AI-forst√•else\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "print(\"üöÄ LLM Grunnleggende - Fra tekst til AI-forst√•else\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb301a75",
   "metadata": {},
   "source": [
    "## üìì Tokenisering: Hvordan AI leser tekst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9243280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tekst: Pasienten har diabetes mellitus type 2 og hypertensjon.\n",
      "Antall tokens: 15\n",
      "Tokens: ['Pas', 'ient', 'en', ' har', ' diabetes', ' mell', 'itus', ' type', ' ', '2', ' og', ' hypert', 'ens', 'jon', '.']\n",
      "Token 0: 'Pas' (ID: 72011)\n",
      "Token 1: 'ient' (ID: 1188)\n",
      "Token 2: 'en' (ID: 268)\n",
      "Token 3: ' har' (ID: 4960)\n",
      "Token 4: ' diabetes' (ID: 20335)\n",
      "Token 5: ' mell' (ID: 54448)\n",
      "Token 6: 'itus' (ID: 36891)\n",
      "Token 7: ' type' (ID: 955)\n",
      "Token 8: ' ' (ID: 220)\n",
      "Token 9: '2' (ID: 17)\n",
      "Token 10: ' og' (ID: 7500)\n",
      "Token 11: ' hypert' (ID: 48855)\n",
      "Token 12: 'ens' (ID: 729)\n",
      "Token 13: 'jon' (ID: 35265)\n",
      "Token 14: '.' (ID: 13)\n"
     ]
    }
   ],
   "source": [
    "# Bruk OpenAI's tokenizer\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Medisinsk eksempel\n",
    "tekst = \"Pasienten har diabetes mellitus type 2 og hypertensjon.\"\n",
    "tokens = encoding.encode(tekst)\n",
    "token_strings = [encoding.decode([token]) for token in tokens]\n",
    "\n",
    "print(f\"Original tekst: {tekst}\")\n",
    "print(f\"Antall tokens: {len(tokens)}\")\n",
    "print(f\"Tokens: {token_strings}\")\n",
    "\n",
    "# Visualiser tokenisering\n",
    "for i, (token, string) in enumerate(zip(tokens, token_strings)):\n",
    "    print(f\"Token {i}: '{string}' (ID: {token})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2fa08e",
   "metadata": {},
   "source": [
    "## Temperature: Kontrollere kreativitet vs presisjon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "925545f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Generering med ulike temperature-verdier:\n",
      "--------------------------------------------------\n",
      "Temperature 0.0: diabetes, diabetes, diabetes, diabetes, diabetes\n",
      "               ‚Ü≥ Deterministisk - velger alltid 'diabetes' (h√∏yest sannsynlighet)\n",
      "\n",
      "Temperature 0.5: diabetes, hodepine, feber, smerter, diabetes\n",
      "               ‚Ü≥ Konservativ - favoriserer sannsynlige ord\n",
      "\n",
      "Temperature 1.0: diabetes, diabetes, hodepine, feber, feber\n",
      "               ‚Ü≥ Balansert - f√∏lger opprinnelig distribusjon\n",
      "\n",
      "Temperature 2.0: diabetes, kreft, hodepine, diabetes, diabetes\n",
      "               ‚Ü≥ Kreativ - mer tilfeldige valg\n",
      "\n",
      "üìä Hvordan temperature endrer sannsynlighetsfordelingen:\n",
      "--------------------------------------------------\n",
      "Opprinnelige sannsynligheter:\n",
      "  diabetes  : 0.30\n",
      "  smerter   : 0.25\n",
      "  feber     : 0.20\n",
      "  hodepine  : 0.15\n",
      "  kreft     : 0.10\n",
      "\n",
      "Med temperature = 0.5 (mer fokusert):\n",
      "  diabetes  : 0.40\n",
      "  smerter   : 0.28\n",
      "  feber     : 0.18\n",
      "  hodepine  : 0.10\n",
      "  kreft     : 0.04\n",
      "\n",
      "Med temperature = 2.0 (mer spredt):\n",
      "  diabetes  : 0.25\n",
      "  smerter   : 0.23\n",
      "  feber     : 0.20\n",
      "  hodepine  : 0.18\n",
      "  kreft     : 0.14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_generation(probs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Simuler hvordan temperature p√•virker tekstgenerering\n",
    "    \"\"\"\n",
    "    # Konverter til NumPy array hvis n√∏dvendig\n",
    "    probs = np.array(probs)\n",
    "    \n",
    "    # Juster sannsynligheter basert p√• temperature\n",
    "    if temperature == 0:\n",
    "        # Deterministisk: velg mest sannsynlige\n",
    "        return np.argmax(probs)\n",
    "    \n",
    "    # Skaler log-probs med temperature\n",
    "    log_probs = np.log(probs + 1e-10) / temperature\n",
    "    # Konverter tilbake til sannsynligheter\n",
    "    scaled_probs = np.exp(log_probs)\n",
    "    scaled_probs = scaled_probs / np.sum(scaled_probs)\n",
    "    \n",
    "    # Sample fra distribusjonen\n",
    "    return np.random.choice(len(probs), p=scaled_probs)\n",
    "\n",
    "# Eksempel: Neste ord etter \"Pasienten har\"\n",
    "mulige_ord = [\"diabetes\", \"smerter\", \"feber\", \"hodepine\", \"kreft\"]\n",
    "sannsynligheter = [0.3, 0.25, 0.2, 0.15, 0.1]\n",
    "\n",
    "print(\"ü§ñ Generering med ulike temperature-verdier:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sett seed for reproduserbare resultater\n",
    "np.random.seed(42)\n",
    "\n",
    "for temp in [0.0, 0.5, 1.0, 2.0]:\n",
    "    valgte_ord = []\n",
    "    for _ in range(5):\n",
    "        idx = simulate_generation(sannsynligheter, temp)\n",
    "        valgte_ord.append(mulige_ord[idx])\n",
    "    \n",
    "    print(f\"Temperature {temp:3.1f}: {', '.join(valgte_ord)}\")\n",
    "    \n",
    "    # Legg til forklaring\n",
    "    if temp == 0.0:\n",
    "        print(\"               ‚Ü≥ Deterministisk - velger alltid 'diabetes' (h√∏yest sannsynlighet)\")\n",
    "    elif temp == 0.5:\n",
    "        print(\"               ‚Ü≥ Konservativ - favoriserer sannsynlige ord\")\n",
    "    elif temp == 1.0:\n",
    "        print(\"               ‚Ü≥ Balansert - f√∏lger opprinnelig distribusjon\")\n",
    "    elif temp == 2.0:\n",
    "        print(\"               ‚Ü≥ Kreativ - mer tilfeldige valg\")\n",
    "    print()\n",
    "\n",
    "# Vis hvordan temperature p√•virker distribusjonen\n",
    "print(\"üìä Hvordan temperature endrer sannsynlighetsfordelingen:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "probs_array = np.array(sannsynligheter)\n",
    "print(\"Opprinnelige sannsynligheter:\")\n",
    "for i, (ord, prob) in enumerate(zip(mulige_ord, sannsynligheter)):\n",
    "    print(f\"  {ord:<10}: {prob:.2f}\")\n",
    "\n",
    "print(\"\\nMed temperature = 0.5 (mer fokusert):\")\n",
    "temp = 0.5\n",
    "log_probs = np.log(probs_array + 1e-10) / temp\n",
    "scaled_probs = np.exp(log_probs)\n",
    "scaled_probs = scaled_probs / np.sum(scaled_probs)\n",
    "for i, (ord, prob) in enumerate(zip(mulige_ord, scaled_probs)):\n",
    "    print(f\"  {ord:<10}: {prob:.2f}\")\n",
    "\n",
    "print(\"\\nMed temperature = 2.0 (mer spredt):\")\n",
    "temp = 2.0\n",
    "log_probs = np.log(probs_array + 1e-10) / temp\n",
    "scaled_probs = np.exp(log_probs)\n",
    "scaled_probs = scaled_probs / np.sum(scaled_probs)\n",
    "for i, (ord, prob) in enumerate(zip(mulige_ord, scaled_probs)):\n",
    "    print(f\"  {ord:<10}: {prob:.2f}\")  # Fixed the f-string formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f68cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè• Token-estimat for ulike dokumenttyper:\n",
      "--------------------------------------------------\n",
      "Kort konsultasjon        : ~200 tokens\n",
      "  ‚úÖ Passer i GPT-3.5 (4K kontekst)\n",
      "\n",
      "Standard journalnotat    : ~500 tokens\n",
      "  ‚úÖ Passer i GPT-3.5 (4K kontekst)\n",
      "\n",
      "Omfattende sykehistorie  : ~2,000 tokens\n",
      "  ‚úÖ Passer i GPT-3.5 (4K kontekst)\n",
      "\n",
      "Full pasientjournal      : ~10,000 tokens\n",
      "  üîÑ Trenger GPT-4 Turbo (32K)\n",
      "\n",
      "Komplett pasientmappe    : ~50,000 tokens\n",
      "  üìö Trenger GPT-4 Turbo (128K)\n",
      "\n",
      "Forskningsrapport        : ~25,000 tokens\n",
      "  üîÑ Trenger GPT-4 Turbo (32K)\n",
      "\n",
      "üìã Eksempel journalnotat:\n",
      "Tekstst√∏rrelse: 908 tegn\n",
      "Estimerte tokens: 341\n",
      "‚úÖ Passer enkelt i alle moderne LLM-er\n"
     ]
    }
   ],
   "source": [
    "# Importer n√∏dvendige biblioteker\n",
    "import tiktoken\n",
    "\n",
    "# Hent encoding for GPT-modeller\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # Brukes av GPT-3.5 og GPT-4\n",
    "\n",
    "# Demonstrer kontekstvindu-begrensning\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"Estimer antall tokens i tekst\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Typiske medisinske dokumenter\n",
    "dokumenter = {\n",
    "    \"Kort konsultasjon\": 200,\n",
    "    \"Standard journalnotat\": 500,\n",
    "    \"Omfattende sykehistorie\": 2000,\n",
    "    \"Full pasientjournal\": 10000,\n",
    "    \"Komplett pasientmappe\": 50000,\n",
    "    \"Forskningsrapport\": 25000\n",
    "}\n",
    "\n",
    "print(\"üè• Token-estimat for ulike dokumenttyper:\")\n",
    "print(\"-\" * 50)\n",
    "for dok_type, tokens in dokumenter.items():\n",
    "    print(f\"{dok_type:<25}: ~{tokens:,} tokens\")\n",
    "    \n",
    "    # Klassifiser basert p√• modell-kapasitet\n",
    "    if tokens <= 4000:\n",
    "        print(f\"  ‚úÖ Passer i GPT-3.5 (4K kontekst)\")\n",
    "    elif tokens <= 8000:\n",
    "        print(f\"  ‚ö†Ô∏è  Trenger GPT-4 (8K) eller deling\")\n",
    "    elif tokens <= 32000:\n",
    "        print(f\"  üîÑ Trenger GPT-4 Turbo (32K)\")\n",
    "    elif tokens <= 128000:\n",
    "        print(f\"  üìö Trenger GPT-4 Turbo (128K)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå M√• deles opp eller sammendras\")\n",
    "    print()\n",
    "\n",
    "# Praktisk eksempel med ekte tekst\n",
    "eksempel_journalnotat = \"\"\"\n",
    "Pasient: 65 √•r gammel mann\n",
    "Henvisning: Utredning av brystsmerter\n",
    "\n",
    "Anamnese:\n",
    "Pasienten har hatt tilbakevendende brystsmerter de siste 3 m√•nedene.\n",
    "Smertene er lokalisert substernalt, str√•ler til venstre arm.\n",
    "Utl√∏ses ved anstrengelse, bedres ved hvile.\n",
    "Ingen kjente hjertesykdommer i familien.\n",
    "R√∏yker 20 sigaretter daglig i 40 √•r.\n",
    "\n",
    "Status:\n",
    "Allmenntilstand god, ikke tungpust i hvile.\n",
    "BT: 150/95, Puls: 75/min regul√¶r\n",
    "Hjertelytting: Normale toner, ingen bilyder\n",
    "Lunger: Normale respirasjonslyd\n",
    "\n",
    "Supplerende unders√∏kelser:\n",
    "EKG: Normalt sinusrytme, ingen ST-forandringer\n",
    "Troponin: <0.01 (normalt)\n",
    "Kolesterol: 6.8 mmol/L (h√∏yt)\n",
    "\n",
    "Vurdering:\n",
    "Sannsynlig stabil angina pectoris\n",
    "Kardiovaskul√¶re risikofaktorer: R√∏yking, hypertensjon, hyperkolesterolemi\n",
    "\n",
    "Plan:\n",
    "1. Stress-EKG for √• bekrefte diagnose\n",
    "2. Ekkokardiografi\n",
    "3. Livsstilsr√•d: r√∏ykeslutt, kostomlegging\n",
    "4. Medisinering: ASA 75mg, statin\n",
    "5. Kontroll om 3 m√•neder\n",
    "\"\"\"\n",
    "\n",
    "faktiske_tokens = estimate_tokens(eksempel_journalnotat)\n",
    "print(f\"üìã Eksempel journalnotat:\")\n",
    "print(f\"Tekstst√∏rrelse: {len(eksempel_journalnotat)} tegn\")\n",
    "print(f\"Estimerte tokens: {faktiske_tokens}\")\n",
    "\n",
    "if faktiske_tokens <= 4000:\n",
    "    print(\"‚úÖ Passer enkelt i alle moderne LLM-er\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kan v√¶re utfordrende for eldre modeller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7455d",
   "metadata": {},
   "source": [
    "## Kontekstvindu og begrensninger\n",
    "\n",
    "LLMs har begrenset \"hukommelse\" (kontekstvindu):\n",
    "- GPT-3.5: ~4,000 tokens\n",
    "- GPT-4: 8,000-128,000 tokens\n",
    "- Claude 3: 200,000 tokens\n",
    "\n",
    "For medisinske journaler betyr dette at vi m√•:\n",
    "1. Prioritere relevant informasjon\n",
    "2. Dele opp lange dokumenter\n",
    "3. Bruke sammendrag for historisk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c319cc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-estimat for ulike dokumenttyper:\n",
      "----------------------------------------\n",
      "Kort konsultasjon: ~200 tokens\n",
      "  ‚úÖ Passer i GPT-3.5\n",
      "Standard journalnotat: ~500 tokens\n",
      "  ‚úÖ Passer i GPT-3.5\n",
      "Omfattende sykehistorie: ~2000 tokens\n",
      "  ‚úÖ Passer i GPT-3.5\n",
      "Full pasientjournal: ~10000 tokens\n",
      "  ‚ùå M√• deles opp eller sammendras\n"
     ]
    }
   ],
   "source": [
    "# Demonstrer kontekstvindu-begrensning\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"Estimer antall tokens i tekst\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Typiske medisinske dokumenter\n",
    "dokumenter = {\n",
    "    \"Kort konsultasjon\": 200,\n",
    "    \"Standard journalnotat\": 500,\n",
    "    \"Omfattende sykehistorie\": 2000,\n",
    "    \"Full pasientjournal\": 10000\n",
    "}\n",
    "\n",
    "print(\"Token-estimat for ulike dokumenttyper:\")\n",
    "print(\"-\" * 40)\n",
    "for dok_type, tokens in dokumenter.items():\n",
    "    print(f\"{dok_type}: ~{tokens} tokens\")\n",
    "    if tokens <= 4000:\n",
    "        print(f\"  ‚úÖ Passer i GPT-3.5\")\n",
    "    elif tokens <= 8000:\n",
    "        print(f\"  ‚ö†Ô∏è  Trenger GPT-4 eller deling\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå M√• deles opp eller sammendras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e620acd0",
   "metadata": {},
   "source": [
    "## üí≠ Refleksjonsoppgave\n",
    "\n",
    "1. Hvorfor er tokenisering viktig for medisinske termer?\n",
    "2. N√•r b√∏r vi bruke lav vs h√∏y temperature i kliniske applikasjoner?\n",
    "3. Hvordan kan vi h√•ndtere lange pasientjournaler med begrenset kontekstvindu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c2854",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd1a7e04",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-helse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
