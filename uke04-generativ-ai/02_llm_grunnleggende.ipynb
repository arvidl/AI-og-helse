{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9940420",
   "metadata": {},
   "source": [
    "## ðŸ““ 02_llm_grunnleggende.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5817709",
   "metadata": {},
   "source": [
    "# ðŸ¤– Store SprÃ¥kmodeller (LLM) - Grunnleggende konsepter\n",
    "\n",
    "## LÃ¦ringsmÃ¥l\n",
    "- ForstÃ¥ hvordan LLM genererer tekst\n",
    "- LÃ¦re om tokens og embeddings\n",
    "- Utforske temperature og sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fad73b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ LLM Grunnleggende - Fra tekst til AI-forstÃ¥else\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "print(\"ðŸš€ LLM Grunnleggende - Fra tekst til AI-forstÃ¥else\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb301a75",
   "metadata": {},
   "source": [
    "## Tokenisering: Hvordan AI leser tekst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9243280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tekst: Pasienten har diabetes mellitus type 2 og hypertensjon.\n",
      "Antall tokens: 15\n",
      "Tokens: ['Pas', 'ient', 'en', ' har', ' diabetes', ' mell', 'itus', ' type', ' ', '2', ' og', ' hypert', 'ens', 'jon', '.']\n",
      "Token 0: 'Pas' (ID: 72011)\n",
      "Token 1: 'ient' (ID: 1188)\n",
      "Token 2: 'en' (ID: 268)\n",
      "Token 3: ' har' (ID: 4960)\n",
      "Token 4: ' diabetes' (ID: 20335)\n",
      "Token 5: ' mell' (ID: 54448)\n",
      "Token 6: 'itus' (ID: 36891)\n",
      "Token 7: ' type' (ID: 955)\n",
      "Token 8: ' ' (ID: 220)\n",
      "Token 9: '2' (ID: 17)\n",
      "Token 10: ' og' (ID: 7500)\n",
      "Token 11: ' hypert' (ID: 48855)\n",
      "Token 12: 'ens' (ID: 729)\n",
      "Token 13: 'jon' (ID: 35265)\n",
      "Token 14: '.' (ID: 13)\n"
     ]
    }
   ],
   "source": [
    "# Bruk OpenAI's tokenizer\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Medisinsk eksempel\n",
    "tekst = \"Pasienten har diabetes mellitus type 2 og hypertensjon.\"\n",
    "tokens = encoding.encode(tekst)\n",
    "token_strings = [encoding.decode([token]) for token in tokens]\n",
    "\n",
    "print(f\"Original tekst: {tekst}\")\n",
    "print(f\"Antall tokens: {len(tokens)}\")\n",
    "print(f\"Tokens: {token_strings}\")\n",
    "\n",
    "# Visualiser tokenisering\n",
    "for i, (token, string) in enumerate(zip(tokens, token_strings)):\n",
    "    print(f\"Token {i}: '{string}' (ID: {token})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2fa08e",
   "metadata": {},
   "source": [
    "## Temperature: Kontrollere kreativitet vs presisjon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "925545f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Generering med ulike temperature-verdier:\n",
      "--------------------------------------------------\n",
      "Temperature 0.0: diabetes, diabetes, diabetes, diabetes, diabetes\n",
      "               â†³ Deterministisk - velger alltid 'diabetes' (hÃ¸yest sannsynlighet)\n",
      "\n",
      "Temperature 0.5: diabetes, hodepine, feber, smerter, diabetes\n",
      "               â†³ Konservativ - favoriserer sannsynlige ord\n",
      "\n",
      "Temperature 1.0: diabetes, diabetes, hodepine, feber, feber\n",
      "               â†³ Balansert - fÃ¸lger opprinnelig distribusjon\n",
      "\n",
      "Temperature 2.0: diabetes, kreft, hodepine, diabetes, diabetes\n",
      "               â†³ Kreativ - mer tilfeldige valg\n",
      "\n",
      "ðŸ“Š Hvordan temperature endrer sannsynlighetsfordelingen:\n",
      "--------------------------------------------------\n",
      "Opprinnelige sannsynligheter:\n",
      "  diabetes  : 0.30\n",
      "  smerter   : 0.25\n",
      "  feber     : 0.20\n",
      "  hodepine  : 0.15\n",
      "  kreft     : 0.10\n",
      "\n",
      "Med temperature = 0.5 (mer fokusert):\n",
      "  diabetes  : 0.40\n",
      "  smerter   : 0.28\n",
      "  feber     : 0.18\n",
      "  hodepine  : 0.10\n",
      "  kreft     : 0.04\n",
      "\n",
      "Med temperature = 2.0 (mer spredt):\n",
      "  diabetes  : 0.25\n",
      "  smerter   : 0.23\n",
      "  feber     : 0.20\n",
      "  hodepine  : 0.18\n",
      "  kreft     : 0.14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_generation(probs, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Simuler hvordan temperature pÃ¥virker tekstgenerering\n",
    "    \"\"\"\n",
    "    # Konverter til NumPy array hvis nÃ¸dvendig\n",
    "    probs = np.array(probs)\n",
    "    \n",
    "    # Juster sannsynligheter basert pÃ¥ temperature\n",
    "    if temperature == 0:\n",
    "        # Deterministisk: velg mest sannsynlige\n",
    "        return np.argmax(probs)\n",
    "    \n",
    "    # Skaler log-probs med temperature\n",
    "    log_probs = np.log(probs + 1e-10) / temperature\n",
    "    # Konverter tilbake til sannsynligheter\n",
    "    scaled_probs = np.exp(log_probs)\n",
    "    scaled_probs = scaled_probs / np.sum(scaled_probs)\n",
    "    \n",
    "    # Sample fra distribusjonen\n",
    "    return np.random.choice(len(probs), p=scaled_probs)\n",
    "\n",
    "# Eksempel: Neste ord etter \"Pasienten har\"\n",
    "mulige_ord = [\"diabetes\", \"smerter\", \"feber\", \"hodepine\", \"kreft\"]\n",
    "sannsynligheter = [0.3, 0.25, 0.2, 0.15, 0.1]\n",
    "\n",
    "print(\"ðŸ¤– Generering med ulike temperature-verdier:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sett seed for reproduserbare resultater\n",
    "np.random.seed(42)\n",
    "\n",
    "for temp in [0.0, 0.5, 1.0, 2.0]:\n",
    "    valgte_ord = []\n",
    "    for _ in range(5):\n",
    "        idx = simulate_generation(sannsynligheter, temp)\n",
    "        valgte_ord.append(mulige_ord[idx])\n",
    "    \n",
    "    print(f\"Temperature {temp:3.1f}: {', '.join(valgte_ord)}\")\n",
    "    \n",
    "    # Legg til forklaring\n",
    "    if temp == 0.0:\n",
    "        print(\"               â†³ Deterministisk - velger alltid 'diabetes' (hÃ¸yest sannsynlighet)\")\n",
    "    elif temp == 0.5:\n",
    "        print(\"               â†³ Konservativ - favoriserer sannsynlige ord\")\n",
    "    elif temp == 1.0:\n",
    "        print(\"               â†³ Balansert - fÃ¸lger opprinnelig distribusjon\")\n",
    "    elif temp == 2.0:\n",
    "        print(\"               â†³ Kreativ - mer tilfeldige valg\")\n",
    "    print()\n",
    "\n",
    "# Vis hvordan temperature pÃ¥virker distribusjonen\n",
    "print(\"ðŸ“Š Hvordan temperature endrer sannsynlighetsfordelingen:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "probs_array = np.array(sannsynligheter)\n",
    "print(\"Opprinnelige sannsynligheter:\")\n",
    "for i, (ord, prob) in enumerate(zip(mulige_ord, sannsynligheter)):\n",
    "    print(f\"  {ord:<10}: {prob:.2f}\")\n",
    "\n",
    "print(\"\\nMed temperature = 0.5 (mer fokusert):\")\n",
    "temp = 0.5\n",
    "log_probs = np.log(probs_array + 1e-10) / temp\n",
    "scaled_probs = np.exp(log_probs)\n",
    "scaled_probs = scaled_probs / np.sum(scaled_probs)\n",
    "for i, (ord, prob) in enumerate(zip(mulige_ord, scaled_probs)):\n",
    "    print(f\"  {ord:<10}: {prob:.2f}\")\n",
    "\n",
    "print(\"\\nMed temperature = 2.0 (mer spredt):\")\n",
    "temp = 2.0\n",
    "log_probs = np.log(probs_array + 1e-10) / temp\n",
    "scaled_probs = np.exp(log_probs)\n",
    "scaled_probs = scaled_probs / np.sum(scaled_probs)\n",
    "for i, (ord, prob) in enumerate(zip(mulige_ord, scaled_probs)):\n",
    "    print(f\"  {ord:<10}: {prob:.2f}\")  # Fixed the f-string formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43f68cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¥ Token-estimat for ulike dokumenttyper:\n",
      "--------------------------------------------------\n",
      "Kort konsultasjon        : ~200 tokens\n",
      "  âœ… Passer i GPT-3.5 (4K kontekst)\n",
      "\n",
      "Standard journalnotat    : ~500 tokens\n",
      "  âœ… Passer i GPT-3.5 (4K kontekst)\n",
      "\n",
      "Omfattende sykehistorie  : ~2,000 tokens\n",
      "  âœ… Passer i GPT-3.5 (4K kontekst)\n",
      "\n",
      "Full pasientjournal      : ~10,000 tokens\n",
      "  ðŸ”„ Trenger GPT-4 Turbo (32K)\n",
      "\n",
      "Komplett pasientmappe    : ~50,000 tokens\n",
      "  ðŸ“š Trenger GPT-4 Turbo (128K)\n",
      "\n",
      "Forskningsrapport        : ~25,000 tokens\n",
      "  ðŸ”„ Trenger GPT-4 Turbo (32K)\n",
      "\n",
      "ðŸ“‹ Eksempel journalnotat:\n",
      "TekststÃ¸rrelse: 908 tegn\n",
      "Estimerte tokens: 341\n",
      "âœ… Passer enkelt i alle moderne LLM-er\n"
     ]
    }
   ],
   "source": [
    "# Importer nÃ¸dvendige biblioteker\n",
    "import tiktoken\n",
    "\n",
    "# Hent encoding for GPT-modeller\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # Brukes av GPT-3.5 og GPT-4\n",
    "\n",
    "# Demonstrer kontekstvindu-begrensning\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"Estimer antall tokens i tekst\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Typiske medisinske dokumenter\n",
    "dokumenter = {\n",
    "    \"Kort konsultasjon\": 200,\n",
    "    \"Standard journalnotat\": 500,\n",
    "    \"Omfattende sykehistorie\": 2000,\n",
    "    \"Full pasientjournal\": 10000,\n",
    "    \"Komplett pasientmappe\": 50000,\n",
    "    \"Forskningsrapport\": 25000\n",
    "}\n",
    "\n",
    "print(\"ðŸ¥ Token-estimat for ulike dokumenttyper:\")\n",
    "print(\"-\" * 50)\n",
    "for dok_type, tokens in dokumenter.items():\n",
    "    print(f\"{dok_type:<25}: ~{tokens:,} tokens\")\n",
    "    \n",
    "    # Klassifiser basert pÃ¥ modell-kapasitet\n",
    "    if tokens <= 4000:\n",
    "        print(f\"  âœ… Passer i GPT-3.5 (4K kontekst)\")\n",
    "    elif tokens <= 8000:\n",
    "        print(f\"  âš ï¸  Trenger GPT-4 (8K) eller deling\")\n",
    "    elif tokens <= 32000:\n",
    "        print(f\"  ðŸ”„ Trenger GPT-4 Turbo (32K)\")\n",
    "    elif tokens <= 128000:\n",
    "        print(f\"  ðŸ“š Trenger GPT-4 Turbo (128K)\")\n",
    "    else:\n",
    "        print(f\"  âŒ MÃ¥ deles opp eller sammendras\")\n",
    "    print()\n",
    "\n",
    "# Praktisk eksempel med ekte tekst\n",
    "eksempel_journalnotat = \"\"\"\n",
    "Pasient: 65 Ã¥r gammel mann\n",
    "Henvisning: Utredning av brystsmerter\n",
    "\n",
    "Anamnese:\n",
    "Pasienten har hatt tilbakevendende brystsmerter de siste 3 mÃ¥nedene.\n",
    "Smertene er lokalisert substernalt, strÃ¥ler til venstre arm.\n",
    "UtlÃ¸ses ved anstrengelse, bedres ved hvile.\n",
    "Ingen kjente hjertesykdommer i familien.\n",
    "RÃ¸yker 20 sigaretter daglig i 40 Ã¥r.\n",
    "\n",
    "Status:\n",
    "Allmenntilstand god, ikke tungpust i hvile.\n",
    "BT: 150/95, Puls: 75/min regulÃ¦r\n",
    "Hjertelytting: Normale toner, ingen bilyder\n",
    "Lunger: Normale respirasjonslyd\n",
    "\n",
    "Supplerende undersÃ¸kelser:\n",
    "EKG: Normalt sinusrytme, ingen ST-forandringer\n",
    "Troponin: <0.01 (normalt)\n",
    "Kolesterol: 6.8 mmol/L (hÃ¸yt)\n",
    "\n",
    "Vurdering:\n",
    "Sannsynlig stabil angina pectoris\n",
    "KardiovaskulÃ¦re risikofaktorer: RÃ¸yking, hypertensjon, hyperkolesterolemi\n",
    "\n",
    "Plan:\n",
    "1. Stress-EKG for Ã¥ bekrefte diagnose\n",
    "2. Ekkokardiografi\n",
    "3. LivsstilsrÃ¥d: rÃ¸ykeslutt, kostomlegging\n",
    "4. Medisinering: ASA 75mg, statin\n",
    "5. Kontroll om 3 mÃ¥neder\n",
    "\"\"\"\n",
    "\n",
    "faktiske_tokens = estimate_tokens(eksempel_journalnotat)\n",
    "print(f\"ðŸ“‹ Eksempel journalnotat:\")\n",
    "print(f\"TekststÃ¸rrelse: {len(eksempel_journalnotat)} tegn\")\n",
    "print(f\"Estimerte tokens: {faktiske_tokens}\")\n",
    "\n",
    "if faktiske_tokens <= 4000:\n",
    "    print(\"âœ… Passer enkelt i alle moderne LLM-er\")\n",
    "else:\n",
    "    print(\"âš ï¸ Kan vÃ¦re utfordrende for eldre modeller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7455d",
   "metadata": {},
   "source": [
    "## Kontekstvindu og begrensninger\n",
    "\n",
    "LLMs har begrenset \"hukommelse\" (kontekstvindu):\n",
    "- GPT-3.5: ~4,000 tokens\n",
    "- GPT-4: 8,000-128,000 tokens\n",
    "- Claude 3: 200,000 tokens\n",
    "\n",
    "For medisinske journaler betyr dette at vi mÃ¥:\n",
    "1. Prioritere relevant informasjon\n",
    "2. Dele opp lange dokumenter\n",
    "3. Bruke sammendrag for historisk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c319cc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-estimat for ulike dokumenttyper:\n",
      "----------------------------------------\n",
      "Kort konsultasjon: ~200 tokens\n",
      "  âœ… Passer i GPT-3.5\n",
      "Standard journalnotat: ~500 tokens\n",
      "  âœ… Passer i GPT-3.5\n",
      "Omfattende sykehistorie: ~2000 tokens\n",
      "  âœ… Passer i GPT-3.5\n",
      "Full pasientjournal: ~10000 tokens\n",
      "  âŒ MÃ¥ deles opp eller sammendras\n"
     ]
    }
   ],
   "source": [
    "# Demonstrer kontekstvindu-begrensning\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"Estimer antall tokens i tekst\"\"\"\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Typiske medisinske dokumenter\n",
    "dokumenter = {\n",
    "    \"Kort konsultasjon\": 200,\n",
    "    \"Standard journalnotat\": 500,\n",
    "    \"Omfattende sykehistorie\": 2000,\n",
    "    \"Full pasientjournal\": 10000\n",
    "}\n",
    "\n",
    "print(\"Token-estimat for ulike dokumenttyper:\")\n",
    "print(\"-\" * 40)\n",
    "for dok_type, tokens in dokumenter.items():\n",
    "    print(f\"{dok_type}: ~{tokens} tokens\")\n",
    "    if tokens <= 4000:\n",
    "        print(f\"  âœ… Passer i GPT-3.5\")\n",
    "    elif tokens <= 8000:\n",
    "        print(f\"  âš ï¸  Trenger GPT-4 eller deling\")\n",
    "    else:\n",
    "        print(f\"  âŒ MÃ¥ deles opp eller sammendras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e620acd0",
   "metadata": {},
   "source": [
    "## ðŸ’­ Refleksjonsoppgave\n",
    "\n",
    "1. Hvorfor er tokenisering viktig for medisinske termer?\n",
    "2. NÃ¥r bÃ¸r vi bruke lav vs hÃ¸y temperature i kliniske applikasjoner?\n",
    "3. Hvordan kan vi hÃ¥ndtere lange pasientjournaler med begrenset kontekstvindu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c2854",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-helse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
