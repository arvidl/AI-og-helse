{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f18e465",
   "metadata": {},
   "source": [
    "## üìì 04_chatgpt_claude_api.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25345ae8",
   "metadata": {},
   "source": [
    "# üîå ChatGPT og Claude API - Praktisk Integrasjon\n",
    "\n",
    "## L√¶ringsm√•l\n",
    "- Sette opp og bruke OpenAI og Anthropic APIs\n",
    "- Implementere feilh√•ndtering og rate limiting\n",
    "- Bygge en enkel medisinsk chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ee093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå API Integrasjon - Koble til ChatGPT og Claude\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "# Last milj√∏variabler fra .env fil\n",
    "load_dotenv()\n",
    "\n",
    "print(\"üîå API Integrasjon - Koble til ChatGPT og Claude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945b465",
   "metadata": {},
   "source": [
    "## Oppsett av API-klienter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4586596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIAssistant:\n",
    "    \"\"\"Wrapper-klasse for b√•de OpenAI og Anthropic\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialiser klienter hvis API-n√∏kler finnes\n",
    "        self.openai_client = None\n",
    "        self.anthropic_client = None\n",
    "        \n",
    "        # Sjekk OpenAI\n",
    "        if os.getenv(\"OPENAI_API_KEY\"):\n",
    "            try:\n",
    "                self.openai_client = OpenAI()\n",
    "                print(\"‚úÖ OpenAI klient initialisert\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå OpenAI feil: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è OpenAI API-n√∏kkel ikke funnet\")\n",
    "            \n",
    "        # Sjekk Anthropic - pr√∏v begge mulige navnene\n",
    "        anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\") or os.getenv(\"CLAUDE_API_KEY\")\n",
    "        if anthropic_key:\n",
    "            try:\n",
    "                # Sett milj√∏variabelen med riktig navn hvis n√∏dvendig\n",
    "                if not os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "                    os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_key\n",
    "                \n",
    "                self.anthropic_client = Anthropic()\n",
    "                print(\"‚úÖ Anthropic klient initialisert\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Anthropic feil: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Anthropic API-n√∏kkel ikke funnet (pr√∏vde ANTHROPIC_API_KEY og CLAUDE_API_KEY)\")\n",
    "    \n",
    "    def chat_gpt(self, prompt: str, model: str = \"gpt-3.5-turbo\", \n",
    "                 temperature: float = 0.7, max_tokens: int = 1000) -> Optional[str]:\n",
    "        \"\"\"Send prompt til ChatGPT\"\"\"\n",
    "        if not self.openai_client:\n",
    "            return \"OpenAI ikke tilgjengelig - sjekk API-n√∏kkel\"\n",
    "        \n",
    "        try:\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"OpenAI feil: {e}\"\n",
    "    \n",
    "    def chat_claude(self, prompt: str, model: str = \"claude-3-5-sonnet-20240620\",  # \"claude-4-sonnet-20250514\",\n",
    "                   temperature: float = 0.7, max_tokens: int = 1000) -> Optional[str]:\n",
    "        \"\"\"Send prompt til Claude\"\"\"\n",
    "        if not self.anthropic_client:\n",
    "            return \"Anthropic ikke tilgjengelig - sjekk API-n√∏kkel\"\n",
    "        \n",
    "        try:\n",
    "            response = self.anthropic_client.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.content[0].text\n",
    "        except Exception as e:\n",
    "            return f\"Claude feil: {e}\"\n",
    "    \n",
    "    def get_response(self, prompt: str, model_type: str = \"gpt\", **kwargs) -> str:\n",
    "        \"\"\"Enhetlig interface for begge modeller\"\"\"\n",
    "        if model_type.lower() in [\"gpt\", \"openai\", \"chatgpt\"]:\n",
    "            return self.chat_gpt(prompt, **kwargs)\n",
    "        elif model_type.lower() in [\"claude\", \"anthropic\"]:\n",
    "            return self.chat_claude(prompt, **kwargs)\n",
    "        else:\n",
    "            return f\"Ukjent modelltype: {model_type}\"\n",
    "    \n",
    "    def list_available_models(self):\n",
    "        \"\"\"Vis tilgjengelige modeller\"\"\"\n",
    "        print(\"ü§ñ TILGJENGELIGE MODELLER:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if self.openai_client:\n",
    "            print(\"OpenAI modeller:\")\n",
    "            openai_models = [\n",
    "                \"gpt-3.5-turbo\",\n",
    "                \"gpt-4\",\n",
    "                \"gpt-4-turbo\", \n",
    "                \"gpt-4o\",\n",
    "                \"gpt-4o-mini\"\n",
    "            ]\n",
    "            for model in openai_models:\n",
    "                print(f\"  ‚Ä¢ {model}\")\n",
    "        \n",
    "        if self.anthropic_client:\n",
    "            print(\"\\nAnthropic modeller:\")\n",
    "            claude_models = [\n",
    "                \"claude-4-sonnet-20250514\",    # Nyeste med god balanse mellom intelligens, hastighet og kostnad\n",
    "                \"claude-3-5-sonnet-20240620\",  # Rask og intelligent\n",
    "                \"claude-3-5-haiku-20241022\",   # Rask og billig\n",
    "                \"claude-3-opus-20240229\",      # Kraftigst (men dyr)\n",
    "            ]\n",
    "            for model in claude_models:\n",
    "                print(f\"  ‚Ä¢ {model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd72c7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DEBUGGING MILJ√òVARIABLER:\n",
      "----------------------------------------\n",
      "OPENAI_API_KEY finnes: Ja\n",
      "ANTHROPIC_API_KEY finnes: Nei\n",
      "CLAUDE_API_KEY finnes: Ja\n",
      "OpenAI n√∏kkel: sk-proj...ypoA\n",
      "Anthropic n√∏kkel: sk-ant-...VQAA\n",
      "\n",
      "üöÄ INITIALISERER AI-ASSISTANT:\n",
      "----------------------------------------\n",
      "‚úÖ OpenAI klient initialisert\n",
      "‚úÖ Anthropic klient initialisert\n",
      "ü§ñ TILGJENGELIGE MODELLER:\n",
      "----------------------------------------\n",
      "OpenAI modeller:\n",
      "  ‚Ä¢ gpt-3.5-turbo\n",
      "  ‚Ä¢ gpt-4\n",
      "  ‚Ä¢ gpt-4-turbo\n",
      "  ‚Ä¢ gpt-4o\n",
      "  ‚Ä¢ gpt-4o-mini\n",
      "\n",
      "Anthropic modeller:\n",
      "  ‚Ä¢ claude-4-sonnet-20250514\n",
      "  ‚Ä¢ claude-3-5-sonnet-20240620\n",
      "  ‚Ä¢ claude-3-5-haiku-20241022\n",
      "  ‚Ä¢ claude-3-opus-20240229\n"
     ]
    }
   ],
   "source": [
    "# Debugging: Sjekk milj√∏variabler\n",
    "print(\"üîç DEBUGGING MILJ√òVARIABLER:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"OPENAI_API_KEY finnes: {'Ja' if os.getenv('OPENAI_API_KEY') else 'Nei'}\")\n",
    "print(f\"ANTHROPIC_API_KEY finnes: {'Ja' if os.getenv('ANTHROPIC_API_KEY') else 'Nei'}\")\n",
    "print(f\"CLAUDE_API_KEY finnes: {'Ja' if os.getenv('CLAUDE_API_KEY') else 'Nei'}\")\n",
    "\n",
    "# Vis de f√∏rste og siste tegnene for sikkerhet\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    print(f\"OpenAI n√∏kkel: {key[:7]}...{key[-4:]}\")\n",
    "\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\") or os.getenv(\"CLAUDE_API_KEY\")\n",
    "if anthropic_key:\n",
    "    print(f\"Anthropic n√∏kkel: {anthropic_key[:7]}...{anthropic_key[-4:]}\")\n",
    "\n",
    "print(\"\\nüöÄ INITIALISERER AI-ASSISTANT:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Initialiser assistent\n",
    "assistant = AIAssistant()\n",
    "\n",
    "# Vis tilgjengelige modeller\n",
    "assistant.list_available_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5126863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ TESTING TILKOBLINGER MED OPPDATERTE MODELLER:\n",
      "----------------------------------------\n",
      "Testing OpenAI (GPT-3.5-turbo)...\n",
      "‚úÖ GPT respons: Hei!\n",
      "Testing Anthropic (claude-3-5-sonnet-20240620) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/_4x26lps6mgg1tr73d7g2bmh0000gp/T/ipykernel_65790/2147774612.py:58: DeprecationWarning: The model 'claude-3-5-sonnet-20240620' is deprecated and will reach end-of-life on October 22, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = self.anthropic_client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Claude respons: Hei\n",
      "\n",
      "üÜö MODELL-SAMMENLIGNING:\n",
      "----------------------------------------\n",
      "üìù Prompt: Forklar kortfattet hva diabetes type 2 er til en pasient.\n",
      "\n",
      "ü§ñ GPT-4o-mini:\n",
      "   Diabetes type 2 er en tilstand hvor kroppen ikke bruker insulin effektivt, noe som f√∏rer til h√∏ye niv√•er av sukker (glukose) i blodet. Insulin er et hormon som hjelper kroppen med √• bruke sukker som energi. Over tid kan h√∏ye blodsukkerniv√•er skade organer og blod√•rer. \n",
      "\n",
      "Typiske symptomer inkluderer √∏kt t√∏rst, hyppig vannlating, tretthet og uskarpt syn. Behandling for diabetes type 2 inkluderer ofte livsstilsendringer som kosthold og trening, og noen ganger medisiner for √• hjelpe med blodsukkerkontroll. Det er viktig √• f√∏lge opp med legen for √• h√•ndtere\n",
      "\n",
      "üé≠ Claude-3.5-Haiku:\n",
      "   Her er et forslag til hvordan du kan forklare diabetes type 2 enkelt og forst√•elig:\n",
      "\n",
      "\"Diabetes type 2 er en sykdom som handler om hvordan kroppen din h√•ndterer sukker. Normalt tar hormonet insulin h√•nd om sukkeret i blodet og s√∏rger for at cellene f√•r energi. Ved diabetes type 2 fungerer ikke insulinet skikkelig. Enten produserer bukspytkjertelen for lite insulin, eller s√• blir cellene mindre f√∏lsomme for insulinet.\n",
      "\n",
      "Dette f√∏rer til at sukker blir v√¶rende i blodet i stedet for √• komme inn\n",
      "\n",
      "üé® claude-3-5-sonnet-20240620:\n",
      "   Her er en enkel forklaring av diabetes type 2 som du kan gi til en pasient:\n",
      "\n",
      "\"Diabetes type 2 er en kronisk tilstand hvor kroppen din har problemer med √• regulere blodsukkeret ditt. \n",
      "\n",
      "Normalt produserer bukspyttkjertelen et hormon som heter insulin. Insulin hjelper cellene i kroppen din med √• ta opp og bruke sukker fra blodet ditt. \n",
      "\n",
      "Ved diabetes type 2 har kroppen din blitt mindre f√∏lsom for insulin, eller bukspyttkjertelen klarer ikke √• produsere nok insulin. Dette f√∏rer til at\n"
     ]
    }
   ],
   "source": [
    "# Test begge tjenester med nye modeller\n",
    "print(\"\\nüß™ TESTING TILKOBLINGER MED OPPDATERTE MODELLER:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "test_prompt = \"Si bare 'Hei' p√• norsk\"\n",
    "\n",
    "if assistant.openai_client:\n",
    "    print(\"Testing OpenAI (GPT-3.5-turbo)...\")\n",
    "    gpt_response = assistant.chat_gpt(test_prompt)\n",
    "    print(f\"‚úÖ GPT respons: {gpt_response}\")\n",
    "\n",
    "if assistant.anthropic_client:\n",
    "    print(\"Testing Anthropic (claude-3-5-sonnet-20240620) ...\")  # (claude-4-sonnet-20250514)...\")\n",
    "    claude_response = assistant.chat_claude(test_prompt)\n",
    "    print(f\"‚úÖ Claude respons: {claude_response}\")\n",
    "\n",
    "# Test med ulike modeller\n",
    "if assistant.openai_client and assistant.anthropic_client:\n",
    "    print(\"\\nüÜö MODELL-SAMMENLIGNING:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    comparison_prompt = \"\"\"Forklar kortfattet hva diabetes type 2 er til en pasient.\"\"\"\n",
    "    \n",
    "    print(\"üìù Prompt:\", comparison_prompt)\n",
    "    print()\n",
    "    \n",
    "    # GPT-4o mini (rask og billig)\n",
    "    print(\"ü§ñ GPT-4o-mini:\")\n",
    "    gpt_mini = assistant.chat_gpt(comparison_prompt, model=\"gpt-4o-mini\", max_tokens=150)\n",
    "    print(f\"   {gpt_mini}\")\n",
    "    print()\n",
    "    \n",
    "    # Claude-3.5-Haiku (rask og billig)\n",
    "    print(\"üé≠ Claude-3.5-Haiku:\")\n",
    "    claude_haiku = assistant.chat_claude(comparison_prompt, model=\"claude-3-5-haiku-20241022\", max_tokens=150)\n",
    "    print(f\"   {claude_haiku}\")\n",
    "    print()\n",
    "    \n",
    "    # claude-3-5-sonnet-20240620   # claude-4-sonnet-20250514 (balansert)\n",
    "    print(\"üé® claude-3-5-sonnet-20240620:\")   # claude-4-sonnet-20250514:\")\n",
    "    claude_sonnet = assistant.chat_claude(comparison_prompt, max_tokens=150)\n",
    "    print(f\"   {claude_sonnet}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "605f2d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° MODELL-ANBEFALINGER:\n",
      "----------------------------------------\n",
      "üöÄ For rask prototyping: gpt-4o-mini, claude-3-5-haiku\n",
      "‚öñÔ∏è For balansert ytelse: gpt-4o, claude-4-sonnet-20250514\n",
      "üß† For komplekse oppgaver: gpt-4, claude-3-opus\n",
      "üí∞ Mest kostnadseffektiv: gpt-3.5-turbo, claude-3-5-haiku\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüí° MODELL-ANBEFALINGER:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"üöÄ For rask prototyping: gpt-4o-mini, claude-3-5-haiku\")\n",
    "print(\"‚öñÔ∏è For balansert ytelse: gpt-4o, claude-4-sonnet-20250514\")  \n",
    "print(\"üß† For komplekse oppgaver: gpt-4, claude-3-opus\")\n",
    "print(\"üí∞ Mest kostnadseffektiv: gpt-3.5-turbo, claude-3-5-haiku\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd491c15",
   "metadata": {},
   "source": [
    "## Sammenlign modeller p√• samme oppgave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65af96bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ ChatGPT svar:\n",
      "----------------------------------------\n",
      "Dine blodpr√∏veresultater viser at du har lavt hemoglobin, lav MCV og lav ferritin. Dette kan tyde p√• jernmangelanemi, som kan skyldes for lite jern i kroppen. Du b√∏r snakke med legen din for √• f√• mer informasjon og eventuelt starte behandling med jerntilskudd og endre kostholdet ditt for √• √∏ke jerninntaket. Det er viktig √• f√∏lge opp med jevnlige blodpr√∏ver for √• sjekke om tilstanden bedrer seg.\n",
      "\n",
      "ü§ñ Claude svar:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1g/_4x26lps6mgg1tr73d7g2bmh0000gp/T/ipykernel_65790/2147774612.py:58: DeprecationWarning: The model 'claude-3-5-sonnet-20240620' is deprecated and will reach end-of-life on October 22, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = self.anthropic_client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disse blodpr√∏veresultatene tyder p√• at du kan ha jernmangelanemi. Det betyr at du har litt lavt jernniv√• i kroppen, noe som p√•virker produksjonen av r√∏de blodceller.\n",
      "\n",
      "Hemoglobin-verdien din er litt lav, noe som kan forklare hvorfor du kanskje f√∏ler deg tr√∏tt eller svak. De lave MCV- og ferritin-verdiene st√∏tter ogs√• at dette skyldes jernmangel.\n",
      "\n",
      "Legen din vil sannsynligvis anbefale jerntilskudd for √• √∏ke jernniv√•et ditt. De kan ogs√• unders√∏ke √•rsakene til jernmangelen, som kan v√¶re relatert til kosthold eller andre helseproblemer. \n",
      "\n",
      "Med riktig behandling kan disse verdiene normaliseres, og du vil sannsynligvis f√∏le deg bedre.\n"
     ]
    }
   ],
   "source": [
    "# Medisinsk case for testing\n",
    "medical_case = \"\"\"\n",
    "Forklar f√∏lgende blodpr√∏veresultater for en pasient p√• en enkel m√•te:\n",
    "- Hemoglobin: 10.2 g/dL (ref: 12-16)\n",
    "- MCV: 72 fL (ref: 80-100)\n",
    "- Ferritin: 8 ng/mL (ref: 15-200)\n",
    "\n",
    "Hva kan dette indikere, og hva b√∏r gj√∏res videre?\n",
    "Svar p√• maks 100 ord, p√• norsk, tilpasset pasienten.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ü§ñ ChatGPT svar:\")\n",
    "print(\"-\" * 40)\n",
    "gpt_response = assistant.chat_gpt(medical_case, temperature=0.3)\n",
    "print(gpt_response)\n",
    "\n",
    "print(\"\\nü§ñ Claude svar:\")\n",
    "print(\"-\" * 40)\n",
    "claude_response = assistant.chat_claude(medical_case, temperature=0.3)\n",
    "print(claude_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9d5da",
   "metadata": {},
   "source": [
    "## Bygge en medisinsk chatbot med kontekst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb94888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalChatbot:\n",
    "    \"\"\"Enkel medisinsk chatbot med samtalehistorikk\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = \"openai\"):\n",
    "        self.model_type = model_type\n",
    "        self.assistant = AIAssistant()\n",
    "        self.conversation_history = []\n",
    "        self.system_prompt = \"\"\"\n",
    "        Du er en hjelpsom medisinsk assistent for helsepersonell.\n",
    "        Du gir informasjon basert p√• beste praksis, men minner om at \n",
    "        dine svar m√• valideres og ikke erstatter klinisk vurdering.\n",
    "        Svar p√• norsk, v√¶r presis og bruk korrekt medisinsk terminologi.\n",
    "        \"\"\"\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"Legg til melding i samtalehistorikk\"\"\"\n",
    "        self.conversation_history.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def chat(self, user_input: str) -> str:\n",
    "        \"\"\"H√•ndter brukerinput og generer respons\"\"\"\n",
    "        self.add_message(\"user\", user_input)\n",
    "        \n",
    "        # Bygg full prompt med historikk\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        messages.extend(self.conversation_history[-10:])  # Maks 10 siste meldinger\n",
    "        \n",
    "        if self.model_type == \"openai\" and self.assistant.openai_client:\n",
    "            try:\n",
    "                response = self.assistant.openai_client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=messages,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                bot_response = response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                bot_response = f\"Feil: {e}\"\n",
    "        else:\n",
    "            # Fallback til enkel prompt hvis OpenAI ikke tilgjengelig\n",
    "            bot_response = \"Simulert svar: Jeg forst√•r sp√∏rsm√•let ditt om \" + user_input[:50]\n",
    "        \n",
    "        self.add_message(\"assistant\", bot_response)\n",
    "        return bot_response\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Nullstill samtalehistorikk\"\"\"\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb5a18a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI klient initialisert\n",
      "‚úÖ Anthropic klient initialisert\n",
      "üí¨ Medisinsk Chatbot Demo\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test chatbot\n",
    "chatbot = MedicalChatbot()\n",
    "\n",
    "print(\"üí¨ Medisinsk Chatbot Demo\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da50ac86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ Bruker: Hva er normale verdier for blodtrykk?\n",
      "ü§ñ Bot: Normale verdier for blodtrykk varierer basert p√• alder, kj√∏nn og andre faktorer. Generelt regnes et normalt blodtrykk √• v√¶re under 120/80 mmHg. \n",
      "\n",
      "- Normalt blodtrykk: Under 120/80 mmHg\n",
      "- H√∏yt normalt blodtrykk: 120-129/80-84 mmHg\n",
      "- Hypertensjon (h√∏yt blodtrykk): 130/80 mmHg og h√∏yere\n",
      "\n",
      "Det er viktig √• merke seg at blodtrykksverdiene kan variere gjennom dagen og kan p√•virkes av aktivitet, stress og andre faktorer. Hvis du har sp√∏rsm√•l om ditt blodtrykk, anbefales det √• kontakte helsepersonell for videre oppf√∏lging.\n",
      "\n",
      "üë§ Bruker: Hvilke faktorer kan p√•virke disse verdiene?\n",
      "ü§ñ Bot: Flere faktorer kan p√•virke blodtrykksverdiene, inkludert:\n",
      "\n",
      "1. Fysisk aktivitet: Trening eller fysisk anstrengelse kan midlertidig √∏ke blodtrykket.\n",
      "2. Stress og f√∏lelser: Sterke f√∏lelser som stress, angst eller sinne kan p√•virke blodtrykket.\n",
      "3. Koffein og tobakk: Inntak av koffeinholdige drikker eller r√∏yking kan √∏ke blodtrykket.\n",
      "4. Kosthold: Et h√∏yt saltinntak eller et kosthold rikt p√• mettet fett kan bidra til h√∏yt blodtrykk.\n",
      "5. V√¶skebalanse: Dehydrering kan f√∏re til lavt blodtrykk, mens overhydrering kan p√•virke blodtrykket.\n",
      "6. Medisiner: Noen medisiner kan p√•virke blodtrykket, enten ved √• √∏ke det eller senke det.\n",
      "7. S√∏vn: S√∏vnmangel eller s√∏vnapn√© kan p√•virke blodtrykket.\n",
      "\n",
      "Det er viktig √• v√¶re oppmerksom p√• disse faktorene og fors√∏ke √• kontrollere dem for √• opprettholde et sunt blodtrykk. Vedvarende h√∏yt blodtrykk b√∏r alltid unders√∏kes av helsepersonell for riktig diagnose og behandling.\n",
      "\n",
      "üë§ Bruker: Hvordan behandles h√∏yt blodtrykk?\n",
      "ü§ñ Bot: Behandling av h√∏yt blodtrykk kan inkludere en kombinasjon av livsstilsendringer og medisiner. Her er noen vanlige behandlingsmetoder:\n",
      "\n",
      "1. Livsstilsendringer:\n",
      "   - Kostholdsendringer: Redusere saltinntak, √∏ke inntaket av frukt, gr√∏nnsaker og fullkorn.\n",
      "   - Fysisk aktivitet: Regelmessig trening kan bidra til √• senke blodtrykket.\n",
      "   - Vektkontroll: √Ö opprettholde en sunn vekt er viktig for √• kontrollere blodtrykket.\n",
      "   - Begrens alkoholinntak: Overdreven alkoholforbruk kan √∏ke blodtrykket.\n",
      "   - Slutte √• r√∏yke: R√∏yking kan √∏ke risikoen for hjerte- og karsykdommer, inkludert h√∏yt blodtrykk.\n",
      "\n",
      "2. Medisiner:\n",
      "   - Vanlige medisiner inkluderer ACE-hemmere, betablokkere, kalsiumkanalblokkere og vanndrivende.\n",
      "   - Valg av medisin avhenger av individuelle faktorer og eventuelle underliggende tilstander.\n",
      "   - Medisiner kan bidra til √• senke blodtrykket ved √• redusere hjertets arbeidsbelastning eller slappe av blod√•rene.\n",
      "\n",
      "Det er viktig √• f√∏lge legens anbefalinger n√•r det gjelder behandling av h√∏yt blodtrykk. Regelmessig oppf√∏lging og eventuelle endringer i behandlingsplanen b√∏r alltid skje i samr√•d med helsepersonell.\n"
     ]
    }
   ],
   "source": [
    "# Simuler en samtale\n",
    "questions = [\n",
    "    \"Hva er normale verdier for blodtrykk?\",\n",
    "    \"Hvilke faktorer kan p√•virke disse verdiene?\",\n",
    "    \"Hvordan behandles h√∏yt blodtrykk?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nüë§ Bruker: {q}\")\n",
    "    response = chatbot.chat(q)\n",
    "    print(f\"ü§ñ Bot: {response}\")\n",
    "    time.sleep(1)  # Unng√• rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425df133",
   "metadata": {},
   "source": [
    "## Rate limiting og feilh√•ndtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f1ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimitedAssistant:\n",
    "    \"\"\"API-klient med rate limiting og retry-logikk\"\"\"\n",
    "    \n",
    "    def __init__(self, requests_per_minute: int = 20):\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.min_time_between_requests = 60 / requests_per_minute\n",
    "        self.last_request_time = 0\n",
    "        self.client = OpenAI() if os.getenv(\"OPENAI_API_KEY\") else None\n",
    "    \n",
    "    def wait_if_needed(self):\n",
    "        \"\"\"Vent hvis n√∏dvendig for √• respektere rate limits\"\"\"\n",
    "        time_since_last = time.time() - self.last_request_time\n",
    "        if time_since_last < self.min_time_between_requests:\n",
    "            time.sleep(self.min_time_between_requests - time_since_last)\n",
    "    \n",
    "    def make_request(self, prompt: str, max_retries: int = 3) -> Optional[str]:\n",
    "        \"\"\"Gj√∏r request med retry-logikk\"\"\"\n",
    "        if not self.client:\n",
    "            return \"API ikke tilgjengelig\"\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                self.wait_if_needed()\n",
    "                \n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.7\n",
    "                )\n",
    "                \n",
    "                self.last_request_time = time.time()\n",
    "                return response.choices[0].message.content\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff\n",
    "                    print(f\"Feil: {e}. Venter {wait_time} sekunder...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    return f\"Feil etter {max_retries} fors√∏k: {e}\"\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27cf26c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Tester rate-limited requests...\n",
      "\n",
      "Request 1: Gi meg en kort definisjon av: ...\n",
      "Respons: Anemi er en tilstand karakterisert ved lavt antall r√∏de blodlegemer eller lav hemoglobin i blodet, n...\n",
      "\n",
      "Request 2: Gi meg en kort definisjon av: ...\n",
      "Respons: Hypertensjon er en tilstand karakterisert ved h√∏yt blodtrykk, der trykket i blod√•rene er h√∏yere enn ...\n",
      "\n",
      "Request 3: Gi meg en kort definisjon av: ...\n",
      "Respons: Diabetes er en kronisk sykdom som p√•virker kroppens evne til √• regulere blodsukkerniv√•et, enten p√• g...\n"
     ]
    }
   ],
   "source": [
    "# Test rate limiting\n",
    "rate_limited = RateLimitedAssistant(requests_per_minute=20)\n",
    "\n",
    "print(\"üîÑ Tester rate-limited requests...\")\n",
    "for i in range(3):\n",
    "    prompt = f\"Gi meg en kort definisjon av: {['Anemi', 'Hypertensjon', 'Diabetes'][i]}\"\n",
    "    print(f\"\\nRequest {i+1}: {prompt[:30]}...\")\n",
    "    response = rate_limited.make_request(prompt)\n",
    "    print(f\"Respons: {response[:100]}...\" if response else \"Ingen respons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75284d1b",
   "metadata": {},
   "source": [
    "## üíæ Lagre og gjenbruke responser\n",
    "\n",
    "For √• spare API-kostnader og forbedre ytelse, \n",
    "kan vi cache responser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f404484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c6bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test caching-funksjonalitet\n",
    "class CachedAssistant:\n",
    "    \"\"\"API-klient med caching av responser\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.assistant = AIAssistant()\n",
    "        self.cache_stats = {\"hits\": 0, \"misses\": 0}\n",
    "    \n",
    "    def get_cache_key(self, prompt: str, model: str, temperature: float = 0.7) -> str:\n",
    "        \"\"\"Generer unik cache-n√∏kkel for prompt\"\"\"\n",
    "        content = f\"{model}:{temperature}:{prompt}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def get_cached_response(self, prompt: str, model: str, temperature: float = 0.7) -> Optional[str]:\n",
    "        \"\"\"Hent respons fra cache hvis den finnes\"\"\"\n",
    "        cache_key = self.get_cache_key(prompt, model, temperature)\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.json\"\n",
    "        \n",
    "        if cache_file.exists():\n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"üì¶ Hentet fra cache: {cache_key[:8]}... (spart API-kall)\")\n",
    "                self.cache_stats[\"hits\"] += 1\n",
    "                return data['response']\n",
    "        \n",
    "        self.cache_stats[\"misses\"] += 1\n",
    "        return None\n",
    "    \n",
    "    def save_to_cache(self, prompt: str, model: str, response: str, temperature: float = 0.7):\n",
    "        \"\"\"Lagre respons i cache\"\"\"\n",
    "        cache_key = self.get_cache_key(prompt, model, temperature)\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.json\"\n",
    "        \n",
    "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'prompt': prompt,\n",
    "                'model': model,\n",
    "                'temperature': temperature,\n",
    "                'response': response,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'cache_key': cache_key\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"üíæ Lagret i cache: {cache_key[:8]}...\")\n",
    "    \n",
    "    def get_response(self, prompt: str, model: str = \"gpt\", temperature: float = 0.7) -> str:\n",
    "        \"\"\"Hent respons - f√∏rst fra cache, s√• fra API\"\"\"\n",
    "        # Pr√∏v cache f√∏rst\n",
    "        cached_response = self.get_cached_response(prompt, model, temperature)\n",
    "        if cached_response:\n",
    "            return cached_response\n",
    "        \n",
    "        # Hvis ikke i cache, hent fra API\n",
    "        print(f\"üåê Henter fra {model} API...\")\n",
    "        \n",
    "        # Kall riktig metode basert p√• modelltype\n",
    "        if model.lower() in [\"gpt\", \"openai\", \"chatgpt\"]:\n",
    "            response = self.assistant.chat_gpt(prompt, temperature=temperature)\n",
    "        elif model.lower() in [\"claude\", \"anthropic\"]:\n",
    "            response = self.assistant.chat_claude(prompt, temperature=temperature)\n",
    "        else:\n",
    "            response = f\"Ukjent modelltype: {model}\"\n",
    "        \n",
    "        # Lagre i cache for fremtidig bruk\n",
    "        self.save_to_cache(prompt, model, response, temperature)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Vis cache-statistikk\"\"\"\n",
    "        total = self.cache_stats[\"hits\"] + self.cache_stats[\"misses\"]\n",
    "        hit_rate = self.cache_stats[\"hits\"] / total * 100 if total > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"cache_hits\": self.cache_stats[\"hits\"],\n",
    "            \"cache_misses\": self.cache_stats[\"misses\"],\n",
    "            \"hit_rate\": f\"{hit_rate:.1f}%\",\n",
    "            \"total_requests\": total,\n",
    "            \"cache_files\": len(list(self.cache_dir.glob(\"*.json\")))\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"T√∏m cache\"\"\"\n",
    "        for cache_file in self.cache_dir.glob(\"*.json\"):\n",
    "            cache_file.unlink()\n",
    "        print(f\"üóëÔ∏è Cache t√∏mt - slettet {len(list(self.cache_dir.glob('*.json')))} filer\")\n",
    "        self.cache_stats = {\"hits\": 0, \"misses\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b67518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI klient initialisert\n",
      "‚úÖ Anthropic klient initialisert\n",
      "üß™ TESTING AV CACHE-FUNKSJONALITET\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test caching-funksjonalitet\n",
    "cached_assistant = CachedAssistant()\n",
    "\n",
    "print(\"üß™ TESTING AV CACHE-FUNKSJONALITET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test-prompt som vi kj√∏rer flere ganger\n",
    "test_prompt = \"\"\"\n",
    "Som erfaren allmennlege, gi en kort vurdering av denne pasienten:\n",
    "\n",
    "Pasient: 65-√•rig mann med diabetes type 2\n",
    "Symptomer: Tretthet, √∏kt t√∏rste, hyppig vannlating siste 2 uker\n",
    "Blodpr√∏ver: HbA1c 9.2%, fastende glukose 15.2 mmol/L\n",
    "\n",
    "Gi anbefaling for behandlingsopptrapping.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e36cd739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ F√∏rste kall (vil hente fra API):\n",
      "üì¶ Hentet fra cache: 4bf3aeae... (spart API-kall)\n",
      "Respons lengde: 484 tegn\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ F√∏rste kall (vil hente fra API):\")\n",
    "response1 = cached_assistant.get_response(test_prompt, \"gpt\", temperature=0.7)\n",
    "print(f\"Respons lengde: {len(response1)} tegn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c9f6902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Andre kall (vil hente fra cache):\n",
      "üì¶ Hentet fra cache: 4bf3aeae... (spart API-kall)\n",
      "Respons lengde: 484 tegn\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîÑ Andre kall (vil hente fra cache):\")\n",
    "response2 = cached_assistant.get_response(test_prompt, \"gpt\", temperature=0.7)\n",
    "print(f\"Respons lengde: {len(response2)} tegn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48426b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Cache-statistikk:\n",
      "  cache_hits: 2\n",
      "  cache_misses: 0\n",
      "  hit_rate: 100.0%\n",
      "  total_requests: 2\n",
      "  cache_files: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìä Cache-statistikk:\")\n",
    "stats = cached_assistant.get_cache_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d3839ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîÑ SAMMENLIGNING MED CACHE\n",
      "==================================================\n",
      "üì± Testing begge modeller:\n",
      "\n",
      "GPT:\n",
      "üì¶ Hentet fra cache: 112aef77... (spart API-kall)\n",
      "  Lengde: 537 tegn\n",
      "\n",
      "CLAUDE:\n",
      "üì¶ Hentet fra cache: c38bba63... (spart API-kall)\n",
      "  Lengde: 1160 tegn\n",
      "\n",
      "üìä Oppdatert cache-statistikk:\n",
      "  cache_hits: 4\n",
      "  cache_misses: 0\n",
      "  hit_rate: 100.0%\n",
      "  total_requests: 4\n",
      "  cache_files: 5\n"
     ]
    }
   ],
   "source": [
    "# Test med forskjellige modeller\n",
    "print(\"\\n\\nüîÑ SAMMENLIGNING MED CACHE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "comparison_prompt = \"Forklar diabetes type 2 p√• en enkel m√•te til en pasient\"\n",
    "\n",
    "print(\"üì± Testing begge modeller:\")\n",
    "for model in [\"gpt\", \"claude\"]:\n",
    "    print(f\"\\n{model.upper()}:\")\n",
    "    response = cached_assistant.get_response(comparison_prompt, model, temperature=0.5)\n",
    "    print(f\"  Lengde: {len(response)} tegn\")\n",
    "\n",
    "print(\"\\nüìä Oppdatert cache-statistikk:\")\n",
    "stats = cached_assistant.get_cache_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f81a20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîí DETERMINISTISK CACHE-TEST (temperature=0)\n",
      "==================================================\n",
      "Testing deterministisk respons (temperature=0):\n",
      "\n",
      "Kj√∏ring 1:\n",
      "üì¶ Hentet fra cache: 5fa5e194... (spart API-kall)\n",
      "  Respons: 1. Hyppig vannlating: Personer med diabetes type 2 kan oppleve √∏kt behov for √• urinere, spesielt om ...\n",
      "\n",
      "Kj√∏ring 2:\n",
      "üì¶ Hentet fra cache: 5fa5e194... (spart API-kall)\n",
      "  Respons: 1. Hyppig vannlating: Personer med diabetes type 2 kan oppleve √∏kt behov for √• urinere, spesielt om ...\n",
      "\n",
      "Kj√∏ring 3:\n",
      "üì¶ Hentet fra cache: 5fa5e194... (spart API-kall)\n",
      "  Respons: 1. Hyppig vannlating: Personer med diabetes type 2 kan oppleve √∏kt behov for √• urinere, spesielt om ...\n",
      "\n",
      "üìä Final cache-statistikk:\n",
      "  cache_hits: 7\n",
      "  cache_misses: 0\n",
      "  hit_rate: 100.0%\n",
      "  total_requests: 7\n",
      "  cache_files: 5\n"
     ]
    }
   ],
   "source": [
    "# Test deterministisk caching (temperature=0)\n",
    "print(\"\\n\\nüîí DETERMINISTISK CACHE-TEST (temperature=0)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "deterministic_prompt = \"List opp 3 hovedsymptomer p√• diabetes type 2\"\n",
    "\n",
    "print(\"Testing deterministisk respons (temperature=0):\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nKj√∏ring {i+1}:\")\n",
    "    response = cached_assistant.get_response(deterministic_prompt, \"gpt\", temperature=0)\n",
    "    print(f\"  Respons: {response[:100]}...\")\n",
    "\n",
    "print(\"\\nüìä Final cache-statistikk:\")\n",
    "stats = cached_assistant.get_cache_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f58748ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üîÑ SAMMENLIGNING MED CACHE\n",
      "==================================================\n",
      "üì± Testing begge modeller:\n",
      "\n",
      "GPT:\n",
      "üì¶ Hentet fra cache: 112aef77... (spart API-kall)\n",
      "  Lengde: 537 tegn\n",
      "\n",
      "CLAUDE:\n",
      "üì¶ Hentet fra cache: c38bba63... (spart API-kall)\n",
      "  Lengde: 1160 tegn\n",
      "\n",
      "üìä Oppdatert cache-statistikk:\n",
      "  cache_hits: 9\n",
      "  cache_misses: 0\n",
      "  hit_rate: 100.0%\n",
      "  total_requests: 9\n",
      "  cache_files: 5\n"
     ]
    }
   ],
   "source": [
    "# Demonstrer forskjell mellom modeller\n",
    "print(\"\\n\\nüîÑ SAMMENLIGNING MED CACHE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "comparison_prompt = \"Forklar diabetes type 2 p√• en enkel m√•te til en pasient\"\n",
    "\n",
    "print(\"üì± Testing begge modeller:\")\n",
    "for model in [\"gpt\", \"claude\"]:\n",
    "    print(f\"\\n{model.upper()}:\")\n",
    "    response = cached_assistant.get_response(comparison_prompt, model, temperature=0.5)\n",
    "    print(f\"  Lengde: {len(response)} tegn\")\n",
    "\n",
    "print(\"\\nüìä Oppdatert cache-statistikk:\")\n",
    "stats = cached_assistant.get_cache_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1549fb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "üõ†Ô∏è CACHE-ADMINISTRASJON\n",
      "==================================================\n",
      "üìÅ Cache-mappe: cache\n",
      "üìÑ Antall cache-filer: 5\n",
      "\n",
      "üîç Cache-filer (f√∏rste 3):\n",
      "  1. 112aef77b3ee7bfcd2caebb12a663f21.json\n",
      "     Model: gpt\n",
      "     Timestamp: 2025-09-10T01:31:49.564975\n",
      "     Prompt (start): Forklar diabetes type 2 p√• en enkel m√•te til en pa...\n",
      "\n",
      "  2. 5fa5e194ea221e15be9a4300a77d72e7.json\n",
      "     Model: gpt\n",
      "     Timestamp: 2025-09-10T01:32:05.053129\n",
      "     Prompt (start): List opp 3 hovedsymptomer p√• diabetes type 2...\n",
      "\n",
      "  3. 4bf3aeaef7a68aea009e5678fbf1bfa8.json\n",
      "     Model: gpt\n",
      "     Timestamp: 2025-09-10T01:31:11.554933\n",
      "     Prompt (start): \n",
      "Som erfaren allmennlege, gi en kort vurdering av ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cache-administrasjon\n",
    "print(\"\\n\\nüõ†Ô∏è CACHE-ADMINISTRASJON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def show_cache_contents():\n",
    "    \"\"\"Vis innhold i cache-mappen\"\"\"\n",
    "    cache_files = list(cached_assistant.cache_dir.glob(\"*.json\"))\n",
    "    print(f\"üìÅ Cache-mappe: {cached_assistant.cache_dir}\")\n",
    "    print(f\"üìÑ Antall cache-filer: {len(cache_files)}\")\n",
    "    \n",
    "    if cache_files:\n",
    "        print(\"\\nüîç Cache-filer (f√∏rste 3):\")\n",
    "        for i, cache_file in enumerate(cache_files[:3]):\n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"  {i+1}. {cache_file.name}\")\n",
    "                print(f\"     Model: {data['model']}\")\n",
    "                print(f\"     Timestamp: {data['timestamp']}\")\n",
    "                print(f\"     Prompt (start): {data['prompt'][:50]}...\")\n",
    "                print()\n",
    "\n",
    "show_cache_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6413f83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° CACHE-OPTIMALISERING TIPS\n",
      "==================================================\n",
      "‚úÖ Bruk temperature=0 for deterministiske responser\n",
      "‚úÖ Normaliser prompts (fjern un√∏dvendige mellomrom)\n",
      "‚úÖ Gruppe lignende sp√∏rsm√•l for bedre cache-utnyttelse\n",
      "‚úÖ Overv√•k cache hit rate - m√•l for >70%\n",
      "‚ö†Ô∏è  V√¶r forsiktig med sensitive data i cache\n",
      "‚ö†Ô∏è  Sett opp cache-utl√∏p for data som blir utdatert\n",
      "üí∞ Cache kan spare betydelige API-kostnader\n"
     ]
    }
   ],
   "source": [
    "# Cache-optimalisering tips\n",
    "print(\"\\nüí° CACHE-OPTIMALISERING TIPS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tips = [\n",
    "    \"‚úÖ Bruk temperature=0 for deterministiske responser\",\n",
    "    \"‚úÖ Normaliser prompts (fjern un√∏dvendige mellomrom)\",\n",
    "    \"‚úÖ Gruppe lignende sp√∏rsm√•l for bedre cache-utnyttelse\",\n",
    "    \"‚úÖ Overv√•k cache hit rate - m√•l for >70%\",\n",
    "    \"‚ö†Ô∏è  V√¶r forsiktig med sensitive data i cache\",\n",
    "    \"‚ö†Ô∏è  Sett opp cache-utl√∏p for data som blir utdatert\",\n",
    "    \"üí∞ Cache kan spare betydelige API-kostnader\"\n",
    "]\n",
    "\n",
    "for tip in tips:\n",
    "    print(tip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19850c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avansert cache-funksjonalitet\n",
    "class AdvancedCachedAssistant(CachedAssistant):\n",
    "    \"\"\"Utvidet cache med TTL og kategorisering\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"cache\", default_ttl_hours: int = 24):\n",
    "        super().__init__(cache_dir)\n",
    "        self.default_ttl_hours = default_ttl_hours\n",
    "    \n",
    "    def is_cache_expired(self, cache_file: Path, ttl_hours: int = None) -> bool:\n",
    "        \"\"\"Sjekk om cache er utl√∏pt\"\"\"\n",
    "        if ttl_hours is None:\n",
    "            ttl_hours = self.default_ttl_hours\n",
    "        \n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            cache_time = datetime.fromisoformat(data['timestamp'])\n",
    "            age_hours = (datetime.now() - cache_time).total_seconds() / 3600\n",
    "            return age_hours > ttl_hours\n",
    "    \n",
    "    def get_cached_response(self, prompt: str, model: str, temperature: float = 0.7, \n",
    "                          ttl_hours: int = None) -> Optional[str]:\n",
    "        \"\"\"Hent fra cache med TTL-sjekk\"\"\"\n",
    "        cache_key = self.get_cache_key(prompt, model, temperature)\n",
    "        cache_file = self.cache_dir / f\"{cache_key}.json\"\n",
    "        \n",
    "        if cache_file.exists():\n",
    "            if self.is_cache_expired(cache_file, ttl_hours):\n",
    "                cache_file.unlink()  # Slett utl√∏pt cache\n",
    "                print(f\"‚è∞ Cache utl√∏pt, slettet: {cache_key[:8]}...\")\n",
    "                self.cache_stats[\"misses\"] += 1\n",
    "                return None\n",
    "            \n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                age_hours = (datetime.now() - datetime.fromisoformat(data['timestamp'])).total_seconds() / 3600\n",
    "                print(f\"üì¶ Fra cache ({age_hours:.1f}t gammel): {cache_key[:8]}...\")\n",
    "                self.cache_stats[\"hits\"] += 1\n",
    "                return data['response']\n",
    "        \n",
    "        self.cache_stats[\"misses\"] += 1\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81927ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ AVANSERT CACHE MED TTL\n",
      "==================================================\n",
      "‚úÖ OpenAI klient initialisert\n",
      "‚úÖ Anthropic klient initialisert\n",
      "üïê Test med TTL (Time To Live):\n",
      "üì¶ Fra cache (0.0t gammel): 7f493b50...\n",
      "Respons: Normalt blodtrykk for en 50-√•ring er vanligvis rundt 120/80 mmHg. Det anbefales imidlertjson at blod...\n",
      "\n",
      "üìä Cache-statistikk:\n",
      "  cache_hits: 1\n",
      "  cache_misses: 0\n",
      "  hit_rate: 100.0%\n",
      "  total_requests: 1\n",
      "  cache_files: 5\n"
     ]
    }
   ],
   "source": [
    "# Test avansert cache\n",
    "print(\"\\nüöÄ AVANSERT CACHE MED TTL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "advanced_cache = AdvancedCachedAssistant(default_ttl_hours=1)\n",
    "\n",
    "# Simuler cache med forskjellige aldre\n",
    "test_prompt_ttl = \"Hva er normal blodtrykk for en 50-√•ring?\"\n",
    "\n",
    "print(\"üïê Test med TTL (Time To Live):\")\n",
    "response = advanced_cache.get_response(test_prompt_ttl, \"gpt\", temperature=0)\n",
    "print(f\"Respons: {response[:100]}...\")\n",
    "\n",
    "print(f\"\\nüìä Cache-statistikk:\")\n",
    "for key, value in advanced_cache.get_cache_stats().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02f9b227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ SAMMENDRAG: Cache-implementering\n",
      "==================================================\n",
      "\n",
      "‚úÖ FORDELER MED CACHING:\n",
      "  ‚Ä¢ Reduserte API-kostnader (kan spare 50-90%)\n",
      "  ‚Ä¢ Raskere responstid for gjentatte sp√∏rsm√•l  \n",
      "  ‚Ä¢ Offline-tilgang til tidligere responser\n",
      "  ‚Ä¢ Reproduserbare resultater for testing\n",
      "\n",
      "‚ö†Ô∏è  VIKTIGE HENSYN:\n",
      "  ‚Ä¢ Sensitive data: Ikke cache personopplysninger\n",
      "  ‚Ä¢ Utdatert informasjon: Bruk TTL for medisinske r√•d\n",
      "  ‚Ä¢ Diskplass: Overv√•k cache-st√∏rrelse\n",
      "  ‚Ä¢ Konsistens: Cache kan skjule modell-oppdateringer\n",
      "\n",
      "üè• BRUK I HELSEVESENET:\n",
      "  ‚Ä¢ Cache generelle medisinske sp√∏rsm√•l\n",
      "  ‚Ä¢ Ikke cache pasient-spesifikke vurderinger\n",
      "  ‚Ä¢ Sett kort TTL for behandlingsretningslinjer\n",
      "  ‚Ä¢ Dokumenter hva som caches for compliance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéØ SAMMENDRAG: Cache-implementering\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "‚úÖ FORDELER MED CACHING:\n",
    "  ‚Ä¢ Reduserte API-kostnader (kan spare 50-90%)\n",
    "  ‚Ä¢ Raskere responstid for gjentatte sp√∏rsm√•l  \n",
    "  ‚Ä¢ Offline-tilgang til tidligere responser\n",
    "  ‚Ä¢ Reproduserbare resultater for testing\n",
    "\n",
    "‚ö†Ô∏è  VIKTIGE HENSYN:\n",
    "  ‚Ä¢ Sensitive data: Ikke cache personopplysninger\n",
    "  ‚Ä¢ Utdatert informasjon: Bruk TTL for medisinske r√•d\n",
    "  ‚Ä¢ Diskplass: Overv√•k cache-st√∏rrelse\n",
    "  ‚Ä¢ Konsistens: Cache kan skjule modell-oppdateringer\n",
    "\n",
    "üè• BRUK I HELSEVESENET:\n",
    "  ‚Ä¢ Cache generelle medisinske sp√∏rsm√•l\n",
    "  ‚Ä¢ Ikke cache pasient-spesifikke vurderinger\n",
    "  ‚Ä¢ Sett kort TTL for behandlingsretningslinjer\n",
    "  ‚Ä¢ Dokumenter hva som caches for compliance\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e343a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-helse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
